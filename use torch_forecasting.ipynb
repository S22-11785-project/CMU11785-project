{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005ff4a5-85ef-4544-986d-129911cea505",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a3fe198-8be1-4866-b2a2-6122ff6e9caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "440d4e90-e690-4902-b592-257e9f6725ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "# import dataset, network to train and metric to optimize\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, QuantileLoss, DeepAR\n",
    "\n",
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64aeedb8-d8b3-4316-8b7f-516e388c471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    !kaggle datasets download -d robikscube/ubiquant-parquet -p /home/ubuntu/data\n",
    "    !unzip -q /home/ubuntu/data/ubiquant-parquet.zip -d /home/ubuntu/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3145d759-207c-411a-a8f5-29a0984ad23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>investment_id</th>\n",
       "      <th>target</th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>...</th>\n",
       "      <th>f_290</th>\n",
       "      <th>f_291</th>\n",
       "      <th>f_292</th>\n",
       "      <th>f_293</th>\n",
       "      <th>f_294</th>\n",
       "      <th>f_295</th>\n",
       "      <th>f_296</th>\n",
       "      <th>f_297</th>\n",
       "      <th>f_298</th>\n",
       "      <th>f_299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.300875</td>\n",
       "      <td>0.932573</td>\n",
       "      <td>0.113691</td>\n",
       "      <td>-0.402206</td>\n",
       "      <td>0.378386</td>\n",
       "      <td>-0.203938</td>\n",
       "      <td>-0.413469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.366028</td>\n",
       "      <td>-1.095620</td>\n",
       "      <td>0.200075</td>\n",
       "      <td>0.819155</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.086764</td>\n",
       "      <td>-1.087009</td>\n",
       "      <td>-1.044826</td>\n",
       "      <td>-0.287605</td>\n",
       "      <td>0.321566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0_2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.231040</td>\n",
       "      <td>0.810802</td>\n",
       "      <td>-0.514115</td>\n",
       "      <td>0.742368</td>\n",
       "      <td>-0.616673</td>\n",
       "      <td>-0.194255</td>\n",
       "      <td>1.771210</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.154193</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.734579</td>\n",
       "      <td>0.819155</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.387617</td>\n",
       "      <td>-1.087009</td>\n",
       "      <td>-0.929529</td>\n",
       "      <td>-0.974060</td>\n",
       "      <td>-0.343624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.568807</td>\n",
       "      <td>0.393974</td>\n",
       "      <td>0.615937</td>\n",
       "      <td>0.567806</td>\n",
       "      <td>-0.607963</td>\n",
       "      <td>0.068883</td>\n",
       "      <td>-1.083155</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.138020</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.551904</td>\n",
       "      <td>-1.220772</td>\n",
       "      <td>-1.060166</td>\n",
       "      <td>-0.219097</td>\n",
       "      <td>-1.087009</td>\n",
       "      <td>-0.612428</td>\n",
       "      <td>-0.113944</td>\n",
       "      <td>0.243608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0_7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.064780</td>\n",
       "      <td>-2.343535</td>\n",
       "      <td>-0.011870</td>\n",
       "      <td>1.874606</td>\n",
       "      <td>-0.606346</td>\n",
       "      <td>-0.586827</td>\n",
       "      <td>-0.815737</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382201</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.266359</td>\n",
       "      <td>-1.220772</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.609113</td>\n",
       "      <td>0.104928</td>\n",
       "      <td>-0.783423</td>\n",
       "      <td>1.151730</td>\n",
       "      <td>-0.773309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0_8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.531940</td>\n",
       "      <td>0.842057</td>\n",
       "      <td>-0.262993</td>\n",
       "      <td>2.330030</td>\n",
       "      <td>-0.583422</td>\n",
       "      <td>-0.618392</td>\n",
       "      <td>-0.742814</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.170365</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.741355</td>\n",
       "      <td>-1.220772</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.588445</td>\n",
       "      <td>0.104928</td>\n",
       "      <td>0.753279</td>\n",
       "      <td>1.345611</td>\n",
       "      <td>-0.737624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0_9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1.505904</td>\n",
       "      <td>0.608855</td>\n",
       "      <td>1.369305</td>\n",
       "      <td>-0.761515</td>\n",
       "      <td>0.865860</td>\n",
       "      <td>-0.359269</td>\n",
       "      <td>-1.835762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333684</td>\n",
       "      <td>-1.095620</td>\n",
       "      <td>-0.335999</td>\n",
       "      <td>0.819155</td>\n",
       "      <td>-1.060166</td>\n",
       "      <td>-0.343812</td>\n",
       "      <td>-1.087009</td>\n",
       "      <td>0.077862</td>\n",
       "      <td>0.142943</td>\n",
       "      <td>-0.055550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0_10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.260731</td>\n",
       "      <td>-1.863797</td>\n",
       "      <td>0.113691</td>\n",
       "      <td>1.573864</td>\n",
       "      <td>-0.598433</td>\n",
       "      <td>-0.569936</td>\n",
       "      <td>0.398784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.821560</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>0.476309</td>\n",
       "      <td>-1.220772</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.434315</td>\n",
       "      <td>1.296864</td>\n",
       "      <td>0.171329</td>\n",
       "      <td>1.051288</td>\n",
       "      <td>-0.745335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0_12</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.469207</td>\n",
       "      <td>0.408954</td>\n",
       "      <td>-0.765238</td>\n",
       "      <td>0.261430</td>\n",
       "      <td>-0.591895</td>\n",
       "      <td>-0.037260</td>\n",
       "      <td>0.668721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.821560</td>\n",
       "      <td>-1.095620</td>\n",
       "      <td>-0.864354</td>\n",
       "      <td>-1.220772</td>\n",
       "      <td>-1.060166</td>\n",
       "      <td>-0.300218</td>\n",
       "      <td>1.296864</td>\n",
       "      <td>-0.779556</td>\n",
       "      <td>0.274961</td>\n",
       "      <td>-0.182520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0_13</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.094525</td>\n",
       "      <td>0.861187</td>\n",
       "      <td>2.373796</td>\n",
       "      <td>-1.148977</td>\n",
       "      <td>0.752205</td>\n",
       "      <td>-0.050502</td>\n",
       "      <td>-2.249047</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.658241</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>0.718282</td>\n",
       "      <td>0.819155</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>4.198117</td>\n",
       "      <td>1.296864</td>\n",
       "      <td>1.854434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.688340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0_14</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.251120</td>\n",
       "      <td>-2.476555</td>\n",
       "      <td>0.239253</td>\n",
       "      <td>2.222353</td>\n",
       "      <td>-0.582276</td>\n",
       "      <td>-0.618236</td>\n",
       "      <td>0.386263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.821560</td>\n",
       "      <td>-1.095620</td>\n",
       "      <td>-0.615709</td>\n",
       "      <td>-1.220772</td>\n",
       "      <td>-1.060166</td>\n",
       "      <td>-0.647769</td>\n",
       "      <td>0.104928</td>\n",
       "      <td>-0.849789</td>\n",
       "      <td>0.805876</td>\n",
       "      <td>-0.820165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 304 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  row_id  time_id  investment_id    target       f_0       f_1       f_2  \\\n",
       "0    0_1        0              1 -0.300875  0.932573  0.113691 -0.402206   \n",
       "1    0_2        0              2 -0.231040  0.810802 -0.514115  0.742368   \n",
       "2    0_6        0              6  0.568807  0.393974  0.615937  0.567806   \n",
       "3    0_7        0              7 -1.064780 -2.343535 -0.011870  1.874606   \n",
       "4    0_8        0              8 -0.531940  0.842057 -0.262993  2.330030   \n",
       "5    0_9        0              9  1.505904  0.608855  1.369305 -0.761515   \n",
       "6   0_10        0             10 -0.260731 -1.863797  0.113691  1.573864   \n",
       "7   0_12        0             12 -0.469207  0.408954 -0.765238  0.261430   \n",
       "8   0_13        0             13  0.094525  0.861187  2.373796 -1.148977   \n",
       "9   0_14        0             14 -0.251120 -2.476555  0.239253  2.222353   \n",
       "\n",
       "        f_3       f_4       f_5  ...     f_290     f_291     f_292     f_293  \\\n",
       "0  0.378386 -0.203938 -0.413469  ...  0.366028 -1.095620  0.200075  0.819155   \n",
       "1 -0.616673 -0.194255  1.771210  ... -0.154193  0.912726 -0.734579  0.819155   \n",
       "2 -0.607963  0.068883 -1.083155  ... -0.138020  0.912726 -0.551904 -1.220772   \n",
       "3 -0.606346 -0.586827 -0.815737  ...  0.382201  0.912726 -0.266359 -1.220772   \n",
       "4 -0.583422 -0.618392 -0.742814  ... -0.170365  0.912726 -0.741355 -1.220772   \n",
       "5  0.865860 -0.359269 -1.835762  ...  0.333684 -1.095620 -0.335999  0.819155   \n",
       "6 -0.598433 -0.569936  0.398784  ...  0.821560  0.912726  0.476309 -1.220772   \n",
       "7 -0.591895 -0.037260  0.668721  ...  0.821560 -1.095620 -0.864354 -1.220772   \n",
       "8  0.752205 -0.050502 -2.249047  ... -0.658241  0.912726  0.718282  0.819155   \n",
       "9 -0.582276 -0.618236  0.386263  ...  0.821560 -1.095620 -0.615709 -1.220772   \n",
       "\n",
       "      f_294     f_295     f_296     f_297     f_298     f_299  \n",
       "0  0.941183 -0.086764 -1.087009 -1.044826 -0.287605  0.321566  \n",
       "1  0.941183 -0.387617 -1.087009 -0.929529 -0.974060 -0.343624  \n",
       "2 -1.060166 -0.219097 -1.087009 -0.612428 -0.113944  0.243608  \n",
       "3  0.941183 -0.609113  0.104928 -0.783423  1.151730 -0.773309  \n",
       "4  0.941183 -0.588445  0.104928  0.753279  1.345611 -0.737624  \n",
       "5 -1.060166 -0.343812 -1.087009  0.077862  0.142943 -0.055550  \n",
       "6  0.941183 -0.434315  1.296864  0.171329  1.051288 -0.745335  \n",
       "7 -1.060166 -0.300218  1.296864 -0.779556  0.274961 -0.182520  \n",
       "8  0.941183  4.198117  1.296864  1.854434  0.000000 -0.688340  \n",
       "9 -1.060166 -0.647769  0.104928 -0.849789  0.805876 -0.820165  \n",
       "\n",
       "[10 rows x 304 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_train = '/home/ubuntu/data'\n",
    "data = pd.read_parquet(os.path.join(dir_train, 'train_low_mem.parquet'))\n",
    "\n",
    "# pytorch-forecasting has no built-in methods for dealing with large dataset\n",
    "# when dataset is large, memory will run out\n",
    "# we only use a small fragment of the data to familiarize us with the package\n",
    "# for manual dealing of large dataset\n",
    "# search 'large datasets' on this page: https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.data.timeseries.TimeSeriesDataSet.html\n",
    "data = data[data['time_id'] <= 200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "918ec4b5-6391-485a-8e57-bb8ec79e5bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "row_id            object\n",
       "time_id            int64\n",
       "investment_id     object\n",
       "target           float32\n",
       "f_0              float32\n",
       "                  ...   \n",
       "f_295            float32\n",
       "f_296            float32\n",
       "f_297            float32\n",
       "f_298            float32\n",
       "f_299            float32\n",
       "Length: 304, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TimeSeriesDataSet requires that \n",
    "# we turn investment_id to str, time_id to int\n",
    "\n",
    "data.investment_id = data.investment_id.astype(str)\n",
    "data.time_id = data.time_id.astype(int)\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04d9eefb-d63a-45de-94a0-3f577642a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_names of features, to \n",
    "f_cols = [f'f_{i}' for i in range(300)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d443343b-9108-47cd-8f98-981434f6abfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d49a6666-e1bb-4760-8a2a-d66a6fdef2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_length = 12\n",
    "max_prediction_length = 1\n",
    "training_cutoff = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0ce314a-40c8-49f5-90f9-14d0097d6cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.9/site-packages/pytorch_forecasting/data/timeseries.py:1238: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 4 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__investment_id': '1478'}, {'__group_id__investment_id': '1980'}, {'__group_id__investment_id': '2060'}, {'__group_id__investment_id': '2208'}]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_id <= training_cutoff],\n",
    "    time_idx='time_id',  # column name of time of observation\n",
    "    target='target',  # column name of target to predict\n",
    "    group_ids=['investment_id'],  # column name(s) for timeseries IDs\n",
    "    max_encoder_length=max_encoder_length,  # how much history to use\n",
    "    max_prediction_length=max_prediction_length,  # how far to predict into future\n",
    "    # covariates static for a timeseries ID\n",
    "    static_categoricals=[],\n",
    "    static_reals=[],\n",
    "    # investment_id as categorical covariates that are known in the future for preddiction\n",
    "    time_varying_known_categoricals=['investment_id'],\n",
    "    # put in f_cols and time_id as real covariates that are known in the future for prediction\n",
    "    time_varying_known_reals = f_cols + ['time_id'],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[],\n",
    "    allow_missing_timesteps=True,\n",
    "    # having add_nan=True in Encoder allows us to predict unseen investments\n",
    "    categorical_encoders = {'__group_id__investment_id': NaNLabelEncoder(add_nan=True), 'investment_id': NaNLabelEncoder(add_nan=True)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52d2432d-2998-44f2-a263-cc71f2f0f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = training.to_dataloader(train=True, batch_size=32, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ba102f-10aa-4289-aed4-dab56ee4fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eea17cbe-ef90-4aef-8243-0fd7b404acf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.9/site-packages/pytorch_forecasting/data/timeseries.py:1238: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 98 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__investment_id': '1036'}, {'__group_id__investment_id': '1041'}, {'__group_id__investment_id': '1060'}, {'__group_id__investment_id': '1102'}, {'__group_id__investment_id': '1129'}, {'__group_id__investment_id': '117'}, {'__group_id__investment_id': '1202'}, {'__group_id__investment_id': '1206'}, {'__group_id__investment_id': '1218'}, {'__group_id__investment_id': '1269'}]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True,\n",
    "    categorical_encoders = {'__group_id__investment_id': NaNLabelEncoder(add_nan=True), 'investment_id': NaNLabelEncoder(add_nan=True)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d07a83e1-8414-49c9-83d6-cb5022f9b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = validation.to_dataloader(train=False, batch_size=32 * 10, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f552fabd-b916-4dce-ae70-27d35f57bc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "training.save(\"training.pkl\")\n",
    "validation.save(\"validation.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9e70186-af53-476c-95a6-f80f77a898c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "/home/ubuntu/miniconda3/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py:91: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ubuntu/miniconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/ubuntu/miniconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "target target has to be real",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m pl\u001b[38;5;241m.\u001b[39mseed_everything(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m      4\u001b[0m     gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# clipping gradients is a hyperparameter and important to prevent divergance\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# of the gradient for recurrent neural networks\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     gradient_clip_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m deepar \u001b[38;5;241m=\u001b[39m \u001b[43mDeepAR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# not meaningful for finding the learning rate but otherwise very important\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.03\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# most important hyperparameter apart from learning rate\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# number of attention heads. Set to up to 4 for large datasets\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;43;03m#    attention_head_size=1,\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# between 0.1 and 0.3 are good values\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;43;03m#    hidden_continuous_size=8,  # set to <= hidden_size\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;43;03m#    output_size=7,  # 7 quantiles by default\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQuantileLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# reduce learning rate if no improvement in validation loss after x epochs\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduce_on_plateau_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of parameters in network: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtft\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1e3\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py:187\u001b[0m, in \u001b[0;36mDeepAR.from_dataset\u001b[0;34m(cls, dataset, allowed_encoder_known_variable_names, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m new_kwargs\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mtarget_normalizer, NaNLabelEncoder) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mtarget_normalizer, MultiNormalizer)\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mall\u001b[39m([\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(normalizer, NaNLabelEncoder) \u001b[38;5;28;01mfor\u001b[39;00m normalizer \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mtarget_normalizer])\n\u001b[1;32m    186\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget(s) should be continuous - categorical targets are not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# todo: remove this restriction\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowed_encoder_known_variable_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_encoder_known_variable_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_kwargs\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py:1474\u001b[0m, in \u001b[0;36mBaseModelWithCovariates.from_dataset\u001b[0;34m(cls, dataset, allowed_encoder_known_variable_names, **kwargs)\u001b[0m\n\u001b[1;32m   1454\u001b[0m new_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m   1455\u001b[0m     static_categoricals\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mstatic_categoricals,\n\u001b[1;32m   1456\u001b[0m     time_varying_categoricals_encoder\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m     categorical_groups\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mvariable_groups,\n\u001b[1;32m   1472\u001b[0m )\n\u001b[1;32m   1473\u001b[0m new_kwargs\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[0;32m-> 1474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py:1797\u001b[0m, in \u001b[0;36mAutoRegressiveBaseModel.from_dataset\u001b[0;34m(cls, dataset, **kwargs)\u001b[0m\n\u001b[1;32m   1794\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m lag \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mset\u001b[39m(lags\u001b[38;5;241m.\u001b[39mget(target, [])), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall target lags in dataset must be the same but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlags\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1796\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_lags\u001b[39m\u001b[38;5;124m\"\u001b[39m, {name: dataset\u001b[38;5;241m.\u001b[39m_get_lagged_names(name) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m lags})\n\u001b[0;32m-> 1797\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py:987\u001b[0m, in \u001b[0;36mBaseModel.from_dataset\u001b[0;34m(cls, dataset, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_transformer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    986\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_transformer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mtarget_normalizer\n\u001b[0;32m--> 987\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m net\u001b[38;5;241m.\u001b[39mdataset_parameters \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mget_parameters()\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mmulti_target:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py:134\u001b[0m, in \u001b[0;36mDeepAR.__init__\u001b[0;34m(self, cell_type, hidden_size, rnn_layers, dropout, static_categoricals, static_reals, time_varying_categoricals_encoder, time_varying_categoricals_decoder, categorical_groups, time_varying_reals_encoder, time_varying_reals_decoder, embedding_sizes, embedding_paddings, embedding_labels, x_reals, x_categoricals, n_validation_samples, n_plotting_samples, target, target_lags, loss, logging_metrics, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_variables) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(to_list(target)) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(lagged_target_names) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_variables\n\u001b[1;32m    132\u001b[0m ) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(lagged_target_names), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoder and decoder variables have to be the same apart from target variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m targeti \u001b[38;5;129;01min\u001b[39;00m to_list(target):\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    135\u001b[0m         targeti \u001b[38;5;129;01min\u001b[39;00m time_varying_reals_encoder\n\u001b[1;32m    136\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtargeti\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has to be real\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# todo: remove this restriction\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(target, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loss, DistributionLoss)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(target, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loss, MultiLoss) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(loss) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(target)\n\u001b[1;32m    139\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber of targets should be equivalent to number of loss metrics\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m rnn_class \u001b[38;5;241m=\u001b[39m get_rnn(cell_type)\n",
      "\u001b[0;31mAssertionError\u001b[0m: target target has to be real"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(42)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "deepar = DeepAR.from_dataset(\n",
    "    training,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "#    attention_head_size=1,\n",
    "    dropout=0.1,  # between 0.1 and 0.3 are good values\n",
    "#    hidden_continuous_size=8,  # set to <= hidden_size\n",
    "#    output_size=7,  # 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "# run into error saying \n",
    "# AssertionError: target target has to be real\n",
    "# working on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dec111d-fd08-4ed8-8244-4d290135eb83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
