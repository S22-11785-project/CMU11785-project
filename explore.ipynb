{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make data loading faster, use the [Parquet version](https://www.kaggle.com/c/ubiquant-market-prediction/discussion/301724), example notebook can be found [here](https://www.kaggle.com/code/robikscube/fast-data-loading-and-low-mem-with-parquet-files/notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I downloaded the data and extracted it to a local_data/archive folder\n",
    "dir_train = '/media/user/12TB1/HanLi/GitHub/CMU11785-project/local_data/archive'\n",
    "train = pd.read_parquet(os.path.join(dir_train, 'train_low_mem.parquet')) # this should take 10~30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3141410 entries, 0 to 3141409\n",
      "Columns: 304 entries, row_id to f_299\n",
      "dtypes: float32(301), object(1), uint16(2)\n",
      "memory usage: 3.6+ GB\n",
      "Unique investment_id: 3579\n",
      "Unique time_id: 1211\n",
      "Unique row_id: 3141410\n"
     ]
    }
   ],
   "source": [
    "train.info()\n",
    "print(f\"Unique investment_id: {len(train['investment_id'].unique())}\")\n",
    "print(f\"Unique time_id: {len(train['time_id'].unique())}\") # 1211 probabliy daily or monthly interval?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# from constants import FEATURES\n",
    "FEATURES = [f'f_{i}' for i in range(300)]\n",
    "\n",
    "def collate_fn(datas):\n",
    "    prems = [torch.randperm(data[0].size(0)) for data in datas]\n",
    "    length = min(data[0].size(0) for data in datas)\n",
    "    return [\n",
    "        torch.stack([d[i][perm][:length] for d, perm in zip(datas, prems)])\n",
    "        for i in range(3)\n",
    "    ]\n",
    "\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, *tensor_lists) -> None:\n",
    "        assert all(len(tensor_lists[0]) == len(\n",
    "            t) for t in tensor_lists), \"Size mismatch between tensor_lists\"\n",
    "        self.tensor_lists = tensor_lists\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(t[index] for t in self.tensor_lists)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor_lists[0])\n",
    "\n",
    "def df_to_input_id(df):\n",
    "    return torch.tensor(df['investment_id'].to_numpy(dtype=np.int16),\n",
    "                        dtype=torch.int)\n",
    "\n",
    "\n",
    "def df_to_input_feat(df):\n",
    "    return torch.tensor(df[FEATURES].to_numpy(),\n",
    "                        dtype=torch.float32)\n",
    "\n",
    "\n",
    "def df_to_target(df):\n",
    "    return torch.tensor(df['target'].to_numpy(),\n",
    "                        dtype=torch.float32)\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    df = pd.read_parquet(path)\n",
    "    groups = df.groupby('time_id')\n",
    "    return [\n",
    "        groups.get_group(v)\n",
    "        for v in df.time_id.unique()\n",
    "    ]\n",
    "\n",
    "def split(df_groupby_time, split_ratios):\n",
    "    ids = [df_to_input_id(df) for df in df_groupby_time]\n",
    "    feats = [df_to_input_feat(df) for df in df_groupby_time]\n",
    "    targets = [df_to_target(df) for df in df_groupby_time]\n",
    "\n",
    "    dataset = MyDataset(ids, feats, targets)\n",
    "\n",
    "    lengths = []\n",
    "    for ratio in split_ratios[:-1]:\n",
    "        lengths.append(int(len(dataset)*ratio))\n",
    "    lengths.append(len(dataset) - sum(lengths))\n",
    "\n",
    "    return random_split(dataset, lengths)\n",
    "\n",
    "\n",
    "class UMPDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        datasets = split(load_data(args.input), args.split_ratios)\n",
    "        if len(datasets) == 3:\n",
    "            self.tr, self.val, self.test = datasets\n",
    "        else:\n",
    "            self.tr, self.val = datasets\n",
    "            self.test = self.val\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.tr, batch_size=self.args.batch_size,\n",
    "                          num_workers=self.args.workers, shuffle=True,\n",
    "                          collate_fn=collate_fn, drop_last=True,\n",
    "                          pin_memory=True)\n",
    "\n",
    "    def _val_dataloader(self, dataset):\n",
    "        return DataLoader(dataset, batch_size=1,\n",
    "                          num_workers=self.args.workers, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self._val_dataloader(self.val)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self._val_dataloader(self.test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52c4a99fb36d68752ce25c6541fc636e9171dab977cfe863248a143161a3b436"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('11785_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
