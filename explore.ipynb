{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make data loading faster, use the [Parquet version](https://www.kaggle.com/c/ubiquant-market-prediction/discussion/301724), example notebook can be found [here](https://www.kaggle.com/code/robikscube/fast-data-loading-and-low-mem-with-parquet-files/notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I downloaded the data and extracted it to a local_data/archive folder\n",
    "dir_train = '/media/user/12TB1/HanLi/GitHub/CMU11785-project/local_data/archive'\n",
    "dir_by_time = '/media/user/12TB1/HanLi/GitHub/CMU11785-project/local_data/by_time'\n",
    "# this should take 10~30 seconds.\n",
    "train = pd.read_parquet(os.path.join(dir_train, 'train_low_mem.parquet'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()\n",
    "print(f\"Unique investment_id: {len(train['investment_id'].unique())}\")\n",
    "# 1211 probabliy daily or monthly interval?\n",
    "print(f\"Unique time_id: {len(train['time_id'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data into by time_id:\n",
    "* feature data into shape=(T, N_features) (1211, 3579*300)\n",
    "* Convert target data into shape=(T, N_targets) (1211, 3579)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cols = [f'f_{i}' for i in range(300)]\n",
    "ls_invest_ids = sorted(list(train['investment_id'].unique()))\n",
    "\n",
    "all_feats_cols = []\n",
    "all_tgts_cols = []\n",
    "for invst_id in ls_invest_ids:\n",
    "    all_feats_cols.extend([f\"{invst_id}_{f}\" for f in f_cols])\n",
    "    all_tgts_cols.append(f\"{invst_id}_target\")\n",
    "\n",
    "ls_all_step_fs = []\n",
    "ls_all_step_tgts = []\n",
    "dict_all_step_map = {} # to map target values at each steps so we can query the predictions\n",
    "for i, df_t in train.groupby('time_id'):\n",
    "    break # comment this for real run\n",
    "    time_id = int(df_t['time_id'].unique())\n",
    "    ls_step_fs = [] # to hold all features at current timestep\n",
    "    ls_step_tgts = [] # to hold all target values at current timestep\n",
    "    dict_step_map = {}\n",
    "    for k, invest_id in enumerate(ls_invest_ids):\n",
    "        if invest_id in df_t['investment_id'].tolist():\n",
    "            # If investment_id is in current timestep, include its features\n",
    "            ls_invest_id_fs = df_t.loc[df_t['investment_id']==invest_id, f_cols].values.tolist()[0]\n",
    "            tgt = df_t.loc[df_t['investment_id']==invest_id, 'target'].values[0]\n",
    "            ls_step_tgts.append(tgt)\n",
    "            row_id = df_t.loc[df_t['investment_id']==invest_id, 'row_id'].values[0]\n",
    "            dict_step_map[row_id] = k\n",
    "        else:\n",
    "            # Otherwise append zeros\n",
    "            ls_invest_id_fs = [0]*300 # all features are empty\n",
    "            ls_step_tgts.append(0) # NOTE: Should we use 0 for empty target values?\n",
    "        ls_step_fs.extend(ls_invest_id_fs)\n",
    "    dict_all_step_map[time_id] = dict_step_map\n",
    "    ls_all_step_fs.append(ls_step_fs)\n",
    "    ls_all_step_tgts.append(ls_step_tgts)\n",
    "\n",
    "df_t = pd.DataFrame(ls_all_step_tgts , columns=all_tgts_cols)\n",
    "df_f = pd.DataFrame(ls_all_step_fs , columns=all_feats_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targets = pd.read_parquet(os.path.join(dir_by_time, 'target_by_time.parquet'))\n",
    "df_targets"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52c4a99fb36d68752ce25c6541fc636e9171dab977cfe863248a143161a3b436"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('11785_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
