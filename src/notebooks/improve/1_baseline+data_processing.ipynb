{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important notes\n",
    "1. Replace pytorch-forecasting TFT's ```__init__.py``` file for proper wandb logging.\n",
    "    Use [this](https://drive.google.com/file/d/1WKCAfUQD-R0MZBlwLl44HBvEeQw9D6_0/view?usp=sharing) to replace ```~/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py```\n",
    "\n",
    "2. Replace pytorch-forecasting TFT's ```__init__.py``` file for Yujie's scored attention.\n",
    "    Use [this](https://github.com/jjjoyce/pytorch-forecasting/blob/master/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py) to replace ```~/python3.9/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/sub_modules.py```\n",
    "\n",
    "\n",
    "TODO: Miao to add PCA and AR process in the create_dataset function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import QuantileLoss, SMAPE, MAE, RMSE, MAPE\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_dir(dir_to_add):\n",
    "    if not os.path.exists(dir_to_add):\n",
    "        os.makedirs(dir_to_add)\n",
    "\n",
    "pl.seed_everything(11785)\n",
    "\n",
    "\n",
    "NUM_WORKERS = 8                                                  # Use 4 for AWS\n",
    "DIR_PROJECT = '/media/user/12TB1/HanLi/GitHub/CMU11785-project/' # Change this\n",
    "DIR_TRAINED = os.path.join(DIR_PROJECT, 'local_trained')         # Don't change\n",
    "DIR_DATA = os.path.join(DIR_PROJECT, 'src/data')                 # Don't change\n",
    "DIR_LOCAL_DATA = os.path.join(DIR_PROJECT, 'local_data')         # Don't change\n",
    "DIR_BYID = os.path.join(DIR_LOCAL_DATA, 'content/databyid')      # Might need to change\n",
    "\n",
    "\n",
    "DIR_TS_DATASET = os.path.join(DIR_LOCAL_DATA, 'tft_246')\n",
    "first_run = False                                                # Set to False if you already created dataset\n",
    "\n",
    "create_dir(DIR_TRAINED)\n",
    "create_dir(DIR_LOCAL_DATA)\n",
    "\n",
    "# Read subset list\n",
    "ls_train_val_ids = pd.read_pickle(os.path.join(DIR_DATA, 'test_246_ids.pkl'))\n",
    "ls_test_ids = random.sample(ls_train_val_ids, 200)\n",
    "\n",
    "# Further downsample (if needed)\n",
    "ls_train_val_ids = random.sample(ls_train_val_ids, 100)\n",
    "ls_test_ids = random.sample(ls_train_val_ids, 50)\n",
    "\n",
    "\n",
    "ARGS = args = {\n",
    "    # ------------------------------- Basic config ------------------------------- #\n",
    "    \"case\": \"Baseline-Han\",                  # case name for logging\n",
    "    'batch_size': 64,\n",
    "    'n_workers' : NUM_WORKERS,\n",
    "    'criterion': {\n",
    "        'quantile': QuantileLoss(),\n",
    "        # 'pearson': Pearson.Pearson(),   # Miao's implementation\n",
    "        'other': None,                    # TODO: check out other loss (e.g., MSE)\n",
    "    },\n",
    "    # ----------------------------------- Data ----------------------------------- #\n",
    "    \"ls_train_val_ids\": ls_train_val_ids,\n",
    "    \"ls_test_ids\": ls_test_ids,\n",
    "    'n_train_val_ids': len(ls_train_val_ids),\n",
    "    'n_test_ids': len(ls_test_ids),\n",
    "    \"pre-processing\": None,               # TODO: Miao to add options: [\"PCA\", \"AR\", \"PCA+AR\"]\n",
    "    \"max_prediction_length\": 3,           # We may try tune this\n",
    "    \"max_encoder_length\": 14,             # We may try tune this\n",
    "    \"dir_scaled_df_pkl\": os.path.join(DIR_TS_DATASET, 'df_246_samples_scaled.pkl'),\n",
    "    \"dir_pca_df_pkl\": os.path.join(DIR_TS_DATASET, 'df_246_samples_scaler->pca.pkl'),\n",
    "    \"dir_ar_df_pkl\": os.path.join(DIR_TS_DATASET, 'df_246_samples_ar.pkl'),\n",
    "    \"dir_pca_ar_df_pkl\": os.path.join(DIR_TS_DATASET, 'df_246_samples_ar->pca.pkl'),\n",
    "    # ------------------------------ Hyperparameters -----------------------------\n",
    "    'lr_s': 2e-1,                         # starting lr\n",
    "    'lstm_layers': 2,\n",
    "    'hidden_size': 256,\n",
    "    'attention_head_size': 2,             # use multihead for large hidden size\n",
    "    'dropout': 0.1,\n",
    "    'hidden_continuous_size': 8,          # set to <= hidden_size\n",
    "    'output_size': 7,                     # 7 quantiles for QuantileLoss by default\n",
    "    'reduce_on_plateau_patience': 4,      # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    'gradient_clip_val': 0.1,\n",
    "    # ---------------------------------- Logging --------------------------------- #\n",
    "    'logging_metrics': [QuantileLoss(), SMAPE(), MAE(), RMSE(), MAPE()],\n",
    "    'log_interval': 5,                    # log every n batches, set to None when try to find best lr\n",
    "    'wandb_entity': '11785_project',\n",
    "    'wandb_project': '11785_tft_improvement',\n",
    "    'wandb_name': 'Han_test_run_111',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pre-processed data\n",
    "import gdown\n",
    "# if first_run:\n",
    "if True:\n",
    "    # Scaled\n",
    "    gdown.download(url='https://drive.google.com/file/d/1PtL4CaSSeg_2TqpNnIEbkP2gk2bkqaAP/view?usp=sharing',\n",
    "                output=args[\"dir_scaled_df_pkl\"])\n",
    "\n",
    "    # PCA\n",
    "    gdown.download(url='https://drive.google.com/file/d/1Ofb-WDs3KECOeu3DZkN9d9wM0Q4Qv0E8/view?usp=sharing',\n",
    "                output=args[\"dir_pca_df_pkl\"])\n",
    "\n",
    "    # AR\n",
    "    gdown.download(url='https://drive.google.com/file/d/1OqmxLF93Vzi1GX0HPTsVSRZ0K3hrcTBn/view?usp=sharing',\n",
    "                output=args[\"dir_ar_df_pkl\"])\n",
    "\n",
    "    # AR+PCA\n",
    "    gdown.download(url='https://drive.google.com/file/d/1GVjg8iPvjv5n6Qb6LECyzSJ9MKR6ptV4/view?usp=sharing',\n",
    "                output=args[\"dir_pca_ar_df_pkl\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset\n",
    "* Set ```first_run = True``` to create dataset, otherwise it will read existing ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = pd.read_pickle('/media/user/12TB1/HanLi/GitHub/CMU11785-project/local_data/tft_246/df_246_samples_ar.pkl')\n",
    "# df_t = pd.read_pickle(args[\"dir_pca_ar_df_pkl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create timeseries dataset\n",
    "# TODO: Miao to add PCA and AR pre-processing steps\n",
    "def prepare_df(\n",
    "    ls_train_val_ids,\n",
    "    ls_test_ids, # Should be a subset of ls_train_val_ids\n",
    "    starting_time_id=400, # Use data after 400 time idx\n",
    "    option='original',\n",
    "    args=ARGS,\n",
    "):\n",
    "    \"\"\" Prepare pandas dataframe for creating TS dataset\n",
    "    \"\"\"\n",
    "    assert set(ls_test_ids).issubset(set(ls_train_val_ids))\n",
    "\n",
    "    print(f\"Reading raw data {len(ls_train_val_ids)} train/val samples and {len(ls_test_ids)} test samples...\")\n",
    "    if option == 'original':\n",
    "        f_cols = [f\"f_{i}\" for i in range(300)]\n",
    "        ls_dfs = []\n",
    "        for id in ls_train_val_ids:\n",
    "            df_f_id = pd.DataFrame(np.load(os.path.join(DIR_BYID, f'feats/{id}.npy')), columns=f_cols)\n",
    "            df_t_id = pd.DataFrame(np.load(os.path.join(DIR_BYID, f'target/{id}.npy')), columns=['target'])\n",
    "            df_f_id['investment_id'] = id\n",
    "            df_id = pd.concat([df_t_id, df_f_id], axis=1)\n",
    "            ls_dfs.append(df_id)\n",
    "\n",
    "        df = pd.concat(ls_dfs).reset_index().rename(columns={'index': 'time_id'})\n",
    "        df = df.sort_values(by=['time_id']) # sort by time before splitting\n",
    "        df = df.loc[df['time_id'] >= starting_time_id]\n",
    "    elif option == 'scaled':\n",
    "        # TODO: Miao to add, read df from pickle\n",
    "        df = pd.read_pickle(ARGS['dir_scaled_df_pkl'])\n",
    "    elif option == 'AR':\n",
    "        # TODO: Miao to add, read df from pickle\n",
    "        df = pd.read_pickle(ARGS['dir_scaled_df_pkl'])\n",
    "    elif option == 'PCA':\n",
    "        # TODO: Miao to add, read df from pickle\n",
    "        df = pd.read_pickle(ARGS['dir_scaled_df_pkl'])\n",
    "    elif option == 'AR+PCA':\n",
    "        # TODO: Miao to add, read df from pickle\n",
    "        df = pd.read_pickle(ARGS['dir_scaled_df_pkl'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def df_to_ts_datasets(\n",
    "    df,\n",
    "    ls_train_val_ids,\n",
    "    ls_test_ids,\n",
    "    max_prediction_length=args['max_prediction_length'],\n",
    "    max_encoder_length=args['max_encoder_length'],\n",
    "):\n",
    "    \"\"\" Create TS dataset from pandas dataframe\n",
    "    \"\"\"\n",
    "    assert set(ls_test_ids).issubset(set(ls_train_val_ids))\n",
    "    f_cols = [f\"f_{i}\" for i in range(300)]\n",
    "    df_train, df_test = train_test_split(df, test_size=0.1, shuffle=False)\n",
    "    df_train, df_val = train_test_split(df_train, test_size=2/9, shuffle=False)\n",
    "    df_test = df_test.loc[df_test['investment_id'].isin(ls_test_ids)].reset_index(drop=True)\n",
    "    df_train['investment_id'] = df_train['investment_id'].astype(str)\n",
    "    df_val['investment_id'] = df_val['investment_id'].astype(str)\n",
    "    df_test['investment_id'] = df_test['investment_id'].astype(str)\n",
    "    print('Dataframes read complete, creating TimeSeriesDataSet...')\n",
    "    \n",
    "    # create the dataset from the pandas dataframe\n",
    "    train_dataset = TimeSeriesDataSet(\n",
    "        df_train,\n",
    "        group_ids=[\"investment_id\"],\n",
    "        target=\"target\",\n",
    "        time_idx=\"time_id\",\n",
    "        min_encoder_length=max_encoder_length // 2,\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        min_prediction_length=1,\n",
    "        max_prediction_length=max_prediction_length,\n",
    "        static_categoricals=[\"investment_id\"],\n",
    "        static_reals=[],\n",
    "        time_varying_known_categoricals=[],\n",
    "        time_varying_known_reals=f_cols,\n",
    "        time_varying_unknown_categoricals=[],\n",
    "        time_varying_unknown_reals=['target'],\n",
    "        target_normalizer=GroupNormalizer( # normalize the targe for each investment_id along corresponding time_idx\n",
    "            groups=[\"investment_id\"], \n",
    "            transformation=None # NOTE: do not use softplus or relu for encoder normalization with DeepAR\n",
    "            # transformation=\"softplus\" # NOTE: do not use softplus or relu for encoder normalization with DeepAR\n",
    "        ),\n",
    "        # Add additional features\n",
    "        add_relative_time_idx=True,\n",
    "        add_target_scales=True,\n",
    "        add_encoder_length=True,\n",
    "    )\n",
    "    val_dataset = TimeSeriesDataSet.from_dataset(train_dataset, df_val, predict=True, stop_randomization=True)\n",
    "    test_dataset = TimeSeriesDataSet.from_dataset(train_dataset, df_test, predict=True, stop_randomization=True)\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "def create_dataset(\n",
    "    ls_train_val_ids,\n",
    "    ls_test_ids, # Should be a subset of ls_train_val_ids\n",
    "    starting_time_id=400, # Use data after 400 time idx\n",
    "    max_prediction_length=args['max_prediction_length'],\n",
    "    max_encoder_length=args['max_encoder_length'],\n",
    "    save_path=DIR_TS_DATASET,\n",
    "    option='original'\n",
    "):\n",
    "    \"\"\" Train : Val : Test = 7 : 2 : 1\n",
    "    \"\"\"\n",
    "    df = prepare_df(\n",
    "        ls_train_val_ids,\n",
    "        ls_test_ids, # Should be a subset of ls_train_val_ids\n",
    "        starting_time_id=400, # Use data after 400 time idx\n",
    "        max_prediction_length=args['max_prediction_length'],\n",
    "        max_encoder_length=args['max_encoder_length'],\n",
    "        option=option,\n",
    "    )\n",
    "    train_dataset, val_dataset, test_dataset = df_to_ts_datasets(\n",
    "        df,\n",
    "        ls_train_val_ids,\n",
    "        ls_test_ids,\n",
    "        max_prediction_length,\n",
    "        max_encoder_length,\n",
    "    )\n",
    "    if save_path is not None:\n",
    "        print(f\"Save datasets with {len(ls_train_val_ids)} train/val samples and {len(ls_test_ids)} test samples...\")\n",
    "        # Save dataset so we can use it next time\n",
    "        create_dir(save_path)\n",
    "        train_dataset.save(os.path.join(save_path, f'tft_train_{len(ls_train_val_ids)}_samples.pf'))\n",
    "        val_dataset.save(os.path.join(save_path, f'tft_val_{len(ls_train_val_ids)}_samples.pf'))\n",
    "        test_dataset.save(os.path.join(save_path, f'tft_test_{len(ls_test_ids)}_samples.pf'))\n",
    "\n",
    "\n",
    "if first_run:\n",
    "    train_dataset, val_dataset, test_dataset = create_dataset(ls_train_val_ids, ls_test_ids)\n",
    "else:\n",
    "    train_dataset = TimeSeriesDataSet.load(os.path.join(DIR_TS_DATASET, f'tft_train_{len(args[\"ls_train_val_ids\"])}_samples.pf'))\n",
    "    val_dataset = TimeSeriesDataSet.load(os.path.join(DIR_TS_DATASET, f'tft_val_{len(args[\"ls_train_val_ids\"])}_samples.pf'))\n",
    "    test_dataset = TimeSeriesDataSet.load(os.path.join(DIR_TS_DATASET, f'tft_test_{len(args[\"ls_test_ids\"])}_samples.pf'))\n",
    "\n",
    "# create dataloaders for model\n",
    "train_dataloader = train_dataset.to_dataloader(train=True, batch_size=args['batch_size'], num_workers=args['n_workers'])\n",
    "val_dataloader = val_dataset.to_dataloader(train=False, batch_size=args['batch_size'], num_workers=args['n_workers'])\n",
    "test_dataloader = test_dataset.to_dataloader(train=False, batch_size=args['batch_size'], num_workers=args['n_workers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "\n",
    "# For saving model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss', # val_SMAPE\n",
    "    dirpath=os.path.join(DIR_TRAINED, f'{args[\"case\"]}_ckpt'), \n",
    "    save_top_k=2, \n",
    "    filename=f'{args[\"case\"]}_'+'{epoch:02d}-{val_loss:.2f}-{val_RMSE:.2f}'\n",
    ")\n",
    "\n",
    "logger = WandbLogger(\n",
    "    log_model=True,\n",
    "    entity=args['wandb_entity'],\n",
    "    project=args['wandb_project'],\n",
    "    name=args['wandb_name'],\n",
    "    reinit=True\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    gpus=1,\n",
    "    weights_summary=\"top\",\n",
    "    gradient_clip_val=args['gradient_clip_val'],\n",
    "    # limit_train_batches=5,  # NOTE: uncomment this line only for debugging\n",
    "    callbacks=[lr_logger, early_stop_callback, checkpoint_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "trainer.logger.log_hyperparams(args)\n",
    "\n",
    "tft_model = TemporalFusionTransformer.from_dataset(\n",
    "    train_dataset,\n",
    "    learning_rate=args[\"lr_s\"],\n",
    "    lstm_layers=args[\"lstm_layers\"],\n",
    "    hidden_size=args[\"hidden_size\"],  # most important hyperparameter apart from learning rate\n",
    "    attention_head_size=args[\"attention_head_size\"], # number of attention heads. Set to up to 4 for large datasets\n",
    "    dropout=args[\"dropout\"],  # between 0.1 and 0.3 are good values\n",
    "    # hidden_continuous_size=args[\"hidden_continuous_size\"],\n",
    "    output_size=args['output_size'],\n",
    "    loss=args['criterion']['quantile'],\n",
    "    log_interval=args['log_interval'],  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    reduce_on_plateau_patience=args['reduce_on_plateau_patience'], # reduce learning rate if no improvement in validation loss after x epochs\n",
    ")\n",
    "\n",
    "# fit network\n",
    "trainer.fit(tft_model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "torch.cuda.empty_cache()\n",
    "# wb_run.finish()\n",
    "wandb.finish()\n",
    "torch.save(tft_model.state_dict(), os.path.join(DIR_TRAINED, f'tft_{args[\"case\"]}_428.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: custom test loop, version by horizon length"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52c4a99fb36d68752ce25c6541fc636e9171dab977cfe863248a143161a3b436"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('11785_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
