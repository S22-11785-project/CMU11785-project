{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try TemporalFusionTransformer\n",
    "* Read: https://towardsdatascience.com/temporal-fusion-transformer-a-primer-on-deep-forecasting-in-python-4eb37f3f3594"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "\n",
    "DIR_PROJECT = '/media/user/12TB1/HanLi/GitHub/CMU11785-project/'\n",
    "sys.path.append(os.path.join(DIR_PROJECT, 'src'))\n",
    "sys.path.append(os.path.join(DIR_PROJECT, 'utils'))\n",
    "DIR_TRAINED = os.path.join(DIR_PROJECT, 'local_trained')\n",
    "\n",
    "from criterions import Pearson\n",
    "\n",
    "DIR_DATA = os.path.join(DIR_PROJECT, 'local_data')\n",
    "NUM_WORKERS = 8 # Use 4 for AWS\n",
    "\n",
    "\n",
    "# Baseline hyperparameters: \n",
    "# TFT_quantile_loss_tune_10\n",
    "# https://wandb.ai/11785_project/11785_project_tuning\n",
    "args = {\n",
    "    # ------------------------------\n",
    "    # Basic config\n",
    "    'random_seed': 11785,\n",
    "    'n_samples': 1000,\n",
    "    'batch_size': 64,\n",
    "    'n_workers' : NUM_WORKERS,\n",
    "    'criterion': {\n",
    "        'quantile': QuantileLoss(),\n",
    "        'pearson': Pearson.Pearson(),   # Miao's implementation\n",
    "        'other': None,                  # TODO: check out other loss (e.g., MSE)\n",
    "    },\n",
    "    # ------------------------------\n",
    "    # Hyperparameters\n",
    "    'lr_s': 7e-3,\n",
    "    'hidden_size': 512,\n",
    "    'attention_head_size': 2,        # use multihead for large hidden size\n",
    "    'dropout': 0.1,\n",
    "    'hidden_continuous_size': 4,     # set to <= hidden_size\n",
    "    'output_size': 7,                # 7 quantiles for QuantileLoss by default\n",
    "    'reduce_on_plateau_patience': 4, # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    'gradient_clip_val': 0.1,\n",
    "    # ------------------------------\n",
    "    # Logging\n",
    "    'log_interval': 5,               # log every n batches, set to None when try to find best lr\n",
    "    'wandb_entity': '11785_project',\n",
    "    'wandb_project': '11785_project_tuning',\n",
    "    'wandb_name': 'TFT_baseline_0426',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, create validation and training dataset\n",
    "dir_pf_dataset = os.path.join(DIR_DATA, 'pf_dataset_tft')\n",
    "n = args['n_samples']\n",
    "\n",
    "train_dataset = TimeSeriesDataSet.load(os.path.join(dir_pf_dataset, f'pf_train_{n}_samples.pf'))\n",
    "val_dataset = TimeSeriesDataSet.load(os.path.join(dir_pf_dataset, f'pf_val_{n}_samples.pf'))\n",
    "test_dataset = TimeSeriesDataSet.load(os.path.join(dir_pf_dataset, f'pf_test_{n}_samples.pf'))\n",
    "test_dataset_f = TimeSeriesDataSet.load('/media/user/12TB1/HanLi/GitHub/CMU11785-project/local_data/pf_dataset_test/pf_test_500_samples.pf')\n",
    "\n",
    "# create dataloaders for model\n",
    "train_dataloader = train_dataset.to_dataloader(train=True, batch_size=args['batch_size'], num_workers=args['n_workers'])\n",
    "val_dataloader = val_dataset.to_dataloader(train=False, batch_size=args['batch_size'], num_workers=args['n_workers'])\n",
    "test_dataloader = test_dataset.to_dataloader(train=False, batch_size=args['batch_size'], num_workers=args['n_workers'])\n",
    "test_dataloader_f = test_dataset.to_dataloader(train=False, batch_size=args['batch_size'], num_workers=args['n_workers'])\n",
    "\n",
    "print(\"Load existing datasets completed:\")\n",
    "print(f\"Train dataset:  pf_train_{n}_samples.pf\")\n",
    "print(f\"Val dataset:    pf_val_{n}_samples.pf\")\n",
    "print(f\"Test dataset:   pf_test_{n}_samples.pf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure network and trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_SMAPE', \n",
    "    dirpath='/media/user/12TB1/HanLi/GitHub/CMU11785-project/logs/model_checkpoints/', \n",
    "    save_top_k=2, \n",
    "    filename='500-default-{epoch:02d}-{val_SMAPE:.2f}'\n",
    ")\n",
    "\n",
    "logger = WandbLogger(\n",
    "    entity=\"11785_project\",\n",
    "    project=\"11785_project_tuning\",\n",
    "    name='TFT_baseline',\n",
    "    log_model=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    gpus=1,\n",
    "    weights_summary=\"top\",\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback, checkpoint_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "\n",
    "tft_model = TemporalFusionTransformer.from_dataset(\n",
    "    train_dataset,\n",
    "    learning_rate=args['lr_s'],\n",
    "    hidden_size=args['hidden_size'],  # most important hyperparameter apart from learning rate\n",
    "    attention_head_size=args['attention_head_size'], # number of attention heads. Set to up to 4 for large datasets\n",
    "    dropout=args['dropout'],  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=args['hidden_continuous_size'],\n",
    "    output_size=args['output_size'],\n",
    "    loss=args['criterion']['quantile'],\n",
    "    # loss=args['criterion']['pearson'],\n",
    "    log_interval=args['log_interval'],  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    reduce_on_plateau_patience=args['reduce_on_plateau_patience'], # reduce learning rate if no improvement in validation loss after x epochs\n",
    ")\n",
    "\n",
    "# fit network\n",
    "trainer.fit(\n",
    "    tft_model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")\n",
    "torch.save(tft_model.state_dict(), os.path.join(DIR_TRAINED, 'tft_baseline.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "==>> inp['encoder_cat'].shape:  torch.Size([64, 14, 1])\n",
      "==>> inp['encoder_cont'].shape:  torch.Size([64, 14, 305])\n",
      "==>> inp['encoder_target'].shape:  torch.Size([64, 14])\n",
      "==>> inp['encoder_lengths'].shape:  torch.Size([64])\n",
      "==>> inp['decoder_cat'].shape:  torch.Size([64, 3, 1])\n",
      "==>> inp['decoder_cont'].shape:  torch.Size([64, 3, 305])\n",
      "==>> inp['decoder_target'].shape:  torch.Size([64, 3])\n",
      "==>> inp['decoder_lengths'].shape:  torch.Size([64])\n",
      "==>> inp['decoder_time_idx'].shape:  torch.Size([64, 3])\n",
      "==>> inp['groups'].shape:  torch.Size([64, 1])\n",
      "==>> inp['target_scale'].shape:  torch.Size([64, 2])\n",
      "==>> target[0].shape:  torch.Size([64, 3])\n",
      "==>> inp['encoder_cont']:  tensor([[[ 1.0000e+00, -1.2641e-01,  9.4686e-01,  ...,  8.7591e-01,\n",
      "          -1.0000e+00,  7.4707e-02],\n",
      "         [ 1.0000e+00, -1.2641e-01,  9.4686e-01,  ...,  3.4293e-01,\n",
      "          -9.2857e-01,  1.9884e+00],\n",
      "         [ 1.0000e+00, -1.2641e-01,  9.4686e-01,  ...,  3.2996e-01,\n",
      "          -8.5714e-01, -5.0365e-01],\n",
      "         ...,\n",
      "         [ 1.0000e+00, -1.2641e-01,  9.4686e-01,  ...,  3.5251e-01,\n",
      "          -2.1429e-01, -2.7703e+00],\n",
      "         [ 1.0000e+00, -1.2641e-01,  9.4686e-01,  ...,  7.6839e-01,\n",
      "          -1.4286e-01,  7.7735e-01],\n",
      "         [ 1.0000e+00, -1.2641e-01,  9.4686e-01,  ...,  8.2234e-01,\n",
      "          -7.1429e-02,  1.3653e+00]],\n",
      "\n",
      "        [[ 1.0000e+00, -1.4745e+00,  5.1797e-01,  ..., -5.9071e-01,\n",
      "          -1.0000e+00,  1.1445e-01],\n",
      "         [ 1.0000e+00, -1.4745e+00,  5.1797e-01,  ..., -5.1846e-01,\n",
      "          -9.2857e-01,  4.8960e-02],\n",
      "         [ 1.0000e+00, -1.4745e+00,  5.1797e-01,  ..., -5.3087e-01,\n",
      "          -8.5714e-01, -2.0760e-01],\n",
      "         ...,\n",
      "         [ 1.0000e+00, -1.4745e+00,  5.1797e-01,  ..., -5.7291e-01,\n",
      "          -2.1429e-01, -1.8541e-01],\n",
      "         [ 1.0000e+00, -1.4745e+00,  5.1797e-01,  ..., -5.8163e-01,\n",
      "          -1.4286e-01,  1.8588e-01],\n",
      "         [ 1.0000e+00, -1.4745e+00,  5.1797e-01,  ..., -5.7025e-01,\n",
      "          -7.1429e-02, -7.8441e-01]],\n",
      "\n",
      "        [[ 1.0000e+00, -9.1957e-01,  4.4818e-01,  ..., -7.6728e-01,\n",
      "          -1.0000e+00,  3.3102e-01],\n",
      "         [ 1.0000e+00, -9.1957e-01,  4.4818e-01,  ..., -7.6484e-01,\n",
      "          -9.2857e-01, -6.9444e-01],\n",
      "         [ 1.0000e+00, -9.1957e-01,  4.4818e-01,  ..., -8.8449e-01,\n",
      "          -8.5714e-01, -3.0946e-01],\n",
      "         ...,\n",
      "         [ 1.0000e+00, -9.1957e-01,  4.4818e-01,  ..., -5.8637e-01,\n",
      "          -2.1429e-01, -1.3329e-01],\n",
      "         [ 1.0000e+00, -9.1957e-01,  4.4818e-01,  ..., -5.8769e-01,\n",
      "          -1.4286e-01, -1.6589e-01],\n",
      "         [ 1.0000e+00, -9.1957e-01,  4.4818e-01,  ..., -5.7650e-01,\n",
      "          -7.1429e-02, -7.3357e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.0000e+00,  4.3887e-01,  1.0252e+00,  ..., -5.0928e-01,\n",
      "          -1.0000e+00,  3.0747e-01],\n",
      "         [ 1.0000e+00,  4.3887e-01,  1.0252e+00,  ..., -4.9512e-01,\n",
      "          -9.2857e-01, -2.5226e-01],\n",
      "         [ 1.0000e+00,  4.3887e-01,  1.0252e+00,  ..., -5.3308e-01,\n",
      "          -8.5714e-01,  8.1327e-01],\n",
      "         ...,\n",
      "         [ 1.0000e+00,  4.3887e-01,  1.0252e+00,  ..., -5.0678e-01,\n",
      "          -2.1429e-01, -5.5569e-01],\n",
      "         [ 1.0000e+00,  4.3887e-01,  1.0252e+00,  ..., -5.0861e-01,\n",
      "          -1.4286e-01, -1.0340e+00],\n",
      "         [ 1.0000e+00,  4.3887e-01,  1.0252e+00,  ..., -4.9493e-01,\n",
      "          -7.1429e-02,  6.5492e-01]],\n",
      "\n",
      "        [[ 1.0000e+00,  6.7556e-01, -2.1345e+00,  ...,  9.5887e-02,\n",
      "          -1.0000e+00,  5.5639e+07],\n",
      "         [ 1.0000e+00,  6.7556e-01, -2.1345e+00,  ...,  1.5750e-01,\n",
      "          -9.2857e-01,  1.4059e+08],\n",
      "         [ 1.0000e+00,  6.7556e-01, -2.1345e+00,  ...,  1.4131e-01,\n",
      "          -8.5714e-01,  1.2669e+08],\n",
      "         ...,\n",
      "         [ 1.0000e+00,  6.7556e-01, -2.1345e+00,  ...,  2.5352e-01,\n",
      "          -2.1429e-01,  1.8400e+08],\n",
      "         [ 1.0000e+00,  6.7556e-01, -2.1345e+00,  ...,  2.4673e-01,\n",
      "          -1.4286e-01,  1.1461e+08],\n",
      "         [ 1.0000e+00,  6.7556e-01, -2.1345e+00,  ...,  2.8423e-01,\n",
      "          -7.1429e-02, -6.5734e+07]],\n",
      "\n",
      "        [[ 1.0000e+00,  6.4100e-01, -1.6570e+00,  ...,  7.3173e-01,\n",
      "          -1.0000e+00, -2.7962e+00],\n",
      "         [ 1.0000e+00,  6.4100e-01, -1.6570e+00,  ...,  6.0131e-01,\n",
      "          -9.2857e-01, -1.3918e+00],\n",
      "         [ 1.0000e+00,  6.4100e-01, -1.6570e+00,  ...,  7.1089e-01,\n",
      "          -8.5714e-01,  1.4344e+00],\n",
      "         ...,\n",
      "         [ 1.0000e+00,  6.4100e-01, -1.6570e+00,  ...,  4.0454e-03,\n",
      "          -2.1429e-01, -1.1108e+01],\n",
      "         [ 1.0000e+00,  6.4100e-01, -1.6570e+00,  ..., -1.1192e-03,\n",
      "          -1.4286e-01, -5.0073e+00],\n",
      "         [ 1.0000e+00,  6.4100e-01, -1.6570e+00,  ...,  2.8568e-02,\n",
      "          -7.1429e-02, -4.2507e+00]]])\n"
     ]
    }
   ],
   "source": [
    "for i_b, data in enumerate(test_dataloader):\n",
    "    inp = data[0]\n",
    "    target = data[1]\n",
    "    print('='*100)\n",
    "    print(\"==>> inp['encoder_cat'].shape: \", inp['encoder_cat'].shape)\n",
    "    print(\"==>> inp['encoder_cont'].shape: \", inp['encoder_cont'].shape)\n",
    "    print(\"==>> inp['encoder_target'].shape: \", inp['encoder_target'].shape)\n",
    "    print(\"==>> inp['encoder_lengths'].shape: \", inp['encoder_lengths'].shape)\n",
    "    print(\"==>> inp['decoder_cat'].shape: \", inp['decoder_cat'].shape)\n",
    "    print(\"==>> inp['decoder_cont'].shape: \", inp['decoder_cont'].shape)\n",
    "    print(\"==>> inp['decoder_target'].shape: \", inp['decoder_target'].shape)\n",
    "    print(\"==>> inp['decoder_lengths'].shape: \", inp['decoder_lengths'].shape)\n",
    "    print(\"==>> inp['decoder_time_idx'].shape: \", inp['decoder_time_idx'].shape)\n",
    "    print(\"==>> inp['groups'].shape: \", inp['groups'].shape)\n",
    "    print(\"==>> inp['target_scale'].shape: \", inp['target_scale'].shape)\n",
    "\n",
    "    print(\"==>> target[0].shape: \", target[0].shape)\n",
    "    # print(\"==>> target[0]: \", target[0])\n",
    "    print(\"==>> inp['encoder_cont']: \", inp['encoder_cont'])\n",
    "\n",
    "    # Make prediction, calculate metric\n",
    "    # Y_hat = model(process(inp))\n",
    "    # loss = criterion(Y_hat, target[0])\n",
    "    if i_b >= 0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "==>> inp['encoder_cat'].shape:  torch.Size([64, 14, 1])\n",
      "==>> inp['encoder_cont'].shape:  torch.Size([64, 14, 305])\n",
      "==>> inp['encoder_target'].shape:  torch.Size([64, 14])\n",
      "==>> inp['encoder_lengths'].shape:  torch.Size([64])\n",
      "==>> inp['decoder_cat'].shape:  torch.Size([64, 3, 1])\n",
      "==>> inp['decoder_cont'].shape:  torch.Size([64, 3, 305])\n",
      "==>> inp['decoder_target'].shape:  torch.Size([64, 3])\n",
      "==>> inp['decoder_lengths'].shape:  torch.Size([64])\n",
      "==>> inp['decoder_time_idx'].shape:  torch.Size([64, 3])\n",
      "==>> inp['groups'].shape:  torch.Size([64, 1])\n",
      "==>> inp['target_scale'].shape:  torch.Size([64, 2])\n",
      "==>> target[0].shape:  torch.Size([64, 3])\n",
      "==>> inp['encoder_cont']:  tensor([[[ 1.0000e+00, -1.2641e-01,  9.4686e-01,  ...,  8.7591e-01,\n",
      "          -1.0000e+00,  7.4707e-02],\n",
      "         [ 1.0000e+00, -1.2641e-01,  9.4686e-01,  ...,  3.4293e-01,\n",
      "          -9.2857e-01,  1.9884e+00],\n",
      "         [ 1.0000e+00, -1.2641e-01,  9.4686e-01,  ...,  3.2996e-01,\n",
      "          -8.5714e-01, -5.0365e-01],\n",
      "         ...,\n",
      "         [ 1.0000e+00, -1.2641e-01,  9.4686e-01,  ...,  3.5251e-01,\n",
      "          -2.1429e-01, -2.7703e+00],\n",
      "         [ 1.0000e+00, -1.2641e-01,  9.4686e-01,  ...,  7.6839e-01,\n",
      "          -1.4286e-01,  7.7735e-01],\n",
      "         [ 1.0000e+00, -1.2641e-01,  9.4686e-01,  ...,  8.2234e-01,\n",
      "          -7.1429e-02,  1.3653e+00]],\n",
      "\n",
      "        [[ 1.0000e+00, -1.4745e+00,  5.1797e-01,  ..., -5.9071e-01,\n",
      "          -1.0000e+00,  1.1445e-01],\n",
      "         [ 1.0000e+00, -1.4745e+00,  5.1797e-01,  ..., -5.1846e-01,\n",
      "          -9.2857e-01,  4.8960e-02],\n",
      "         [ 1.0000e+00, -1.4745e+00,  5.1797e-01,  ..., -5.3087e-01,\n",
      "          -8.5714e-01, -2.0760e-01],\n",
      "         ...,\n",
      "         [ 1.0000e+00, -1.4745e+00,  5.1797e-01,  ..., -5.7291e-01,\n",
      "          -2.1429e-01, -1.8541e-01],\n",
      "         [ 1.0000e+00, -1.4745e+00,  5.1797e-01,  ..., -5.8163e-01,\n",
      "          -1.4286e-01,  1.8588e-01],\n",
      "         [ 1.0000e+00, -1.4745e+00,  5.1797e-01,  ..., -5.7025e-01,\n",
      "          -7.1429e-02, -7.8441e-01]],\n",
      "\n",
      "        [[ 1.0000e+00, -9.1957e-01,  4.4818e-01,  ..., -7.6728e-01,\n",
      "          -1.0000e+00,  3.3102e-01],\n",
      "         [ 1.0000e+00, -9.1957e-01,  4.4818e-01,  ..., -7.6484e-01,\n",
      "          -9.2857e-01, -6.9444e-01],\n",
      "         [ 1.0000e+00, -9.1957e-01,  4.4818e-01,  ..., -8.8449e-01,\n",
      "          -8.5714e-01, -3.0946e-01],\n",
      "         ...,\n",
      "         [ 1.0000e+00, -9.1957e-01,  4.4818e-01,  ..., -5.8637e-01,\n",
      "          -2.1429e-01, -1.3329e-01],\n",
      "         [ 1.0000e+00, -9.1957e-01,  4.4818e-01,  ..., -5.8769e-01,\n",
      "          -1.4286e-01, -1.6589e-01],\n",
      "         [ 1.0000e+00, -9.1957e-01,  4.4818e-01,  ..., -5.7650e-01,\n",
      "          -7.1429e-02, -7.3357e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.0000e+00,  4.3887e-01,  1.0252e+00,  ..., -5.0928e-01,\n",
      "          -1.0000e+00,  3.0747e-01],\n",
      "         [ 1.0000e+00,  4.3887e-01,  1.0252e+00,  ..., -4.9512e-01,\n",
      "          -9.2857e-01, -2.5226e-01],\n",
      "         [ 1.0000e+00,  4.3887e-01,  1.0252e+00,  ..., -5.3308e-01,\n",
      "          -8.5714e-01,  8.1327e-01],\n",
      "         ...,\n",
      "         [ 1.0000e+00,  4.3887e-01,  1.0252e+00,  ..., -5.0678e-01,\n",
      "          -2.1429e-01, -5.5569e-01],\n",
      "         [ 1.0000e+00,  4.3887e-01,  1.0252e+00,  ..., -5.0861e-01,\n",
      "          -1.4286e-01, -1.0340e+00],\n",
      "         [ 1.0000e+00,  4.3887e-01,  1.0252e+00,  ..., -4.9493e-01,\n",
      "          -7.1429e-02,  6.5492e-01]],\n",
      "\n",
      "        [[ 1.0000e+00,  6.7556e-01, -2.1345e+00,  ...,  9.5887e-02,\n",
      "          -1.0000e+00,  5.5639e+07],\n",
      "         [ 1.0000e+00,  6.7556e-01, -2.1345e+00,  ...,  1.5750e-01,\n",
      "          -9.2857e-01,  1.4059e+08],\n",
      "         [ 1.0000e+00,  6.7556e-01, -2.1345e+00,  ...,  1.4131e-01,\n",
      "          -8.5714e-01,  1.2669e+08],\n",
      "         ...,\n",
      "         [ 1.0000e+00,  6.7556e-01, -2.1345e+00,  ...,  2.5352e-01,\n",
      "          -2.1429e-01,  1.8400e+08],\n",
      "         [ 1.0000e+00,  6.7556e-01, -2.1345e+00,  ...,  2.4673e-01,\n",
      "          -1.4286e-01,  1.1461e+08],\n",
      "         [ 1.0000e+00,  6.7556e-01, -2.1345e+00,  ...,  2.8423e-01,\n",
      "          -7.1429e-02, -6.5734e+07]],\n",
      "\n",
      "        [[ 1.0000e+00,  6.4100e-01, -1.6570e+00,  ...,  7.3173e-01,\n",
      "          -1.0000e+00, -2.7962e+00],\n",
      "         [ 1.0000e+00,  6.4100e-01, -1.6570e+00,  ...,  6.0131e-01,\n",
      "          -9.2857e-01, -1.3918e+00],\n",
      "         [ 1.0000e+00,  6.4100e-01, -1.6570e+00,  ...,  7.1089e-01,\n",
      "          -8.5714e-01,  1.4344e+00],\n",
      "         ...,\n",
      "         [ 1.0000e+00,  6.4100e-01, -1.6570e+00,  ...,  4.0454e-03,\n",
      "          -2.1429e-01, -1.1108e+01],\n",
      "         [ 1.0000e+00,  6.4100e-01, -1.6570e+00,  ..., -1.1192e-03,\n",
      "          -1.4286e-01, -5.0073e+00],\n",
      "         [ 1.0000e+00,  6.4100e-01, -1.6570e+00,  ...,  2.8568e-02,\n",
      "          -7.1429e-02, -4.2507e+00]]])\n"
     ]
    }
   ],
   "source": [
    "for i_b, data in enumerate(test_dataloader_f):\n",
    "    inp = data[0]\n",
    "    target = data[1]\n",
    "    print('='*100)\n",
    "    print(\"==>> inp['encoder_cat'].shape: \", inp['encoder_cat'].shape)\n",
    "    print(\"==>> inp['encoder_cont'].shape: \", inp['encoder_cont'].shape)\n",
    "    print(\"==>> inp['encoder_target'].shape: \", inp['encoder_target'].shape)\n",
    "    print(\"==>> inp['encoder_lengths'].shape: \", inp['encoder_lengths'].shape)\n",
    "    print(\"==>> inp['decoder_cat'].shape: \", inp['decoder_cat'].shape)\n",
    "    print(\"==>> inp['decoder_cont'].shape: \", inp['decoder_cont'].shape)\n",
    "    print(\"==>> inp['decoder_target'].shape: \", inp['decoder_target'].shape)\n",
    "    print(\"==>> inp['decoder_lengths'].shape: \", inp['decoder_lengths'].shape)\n",
    "    print(\"==>> inp['decoder_time_idx'].shape: \", inp['decoder_time_idx'].shape)\n",
    "    print(\"==>> inp['groups'].shape: \", inp['groups'].shape)\n",
    "    print(\"==>> inp['target_scale'].shape: \", inp['target_scale'].shape)\n",
    "\n",
    "    print(\"==>> target[0].shape: \", target[0].shape)\n",
    "    # print(\"==>> target[0]: \", target[0])\n",
    "    print(\"==>> inp['encoder_cont']: \", inp['encoder_cont'])\n",
    "    # Make prediction, calculate metric\n",
    "    # Y_hat = model(process(inp))\n",
    "    # loss = criterion(Y_hat, target[0])\n",
    "    if i_b >= 0: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tft_model = TemporalFusionTransformer.from_dataset(\n",
    "    test_dataset,\n",
    "    learning_rate=args['lr_s'],\n",
    "    hidden_size=args['hidden_size'],  # most important hyperparameter apart from learning rate\n",
    "    attention_head_size=args['attention_head_size'], # number of attention heads. Set to up to 4 for large datasets\n",
    "    dropout=args['dropout'],  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=args['hidden_continuous_size'],\n",
    "    output_size=args['output_size'],\n",
    "    loss=args['criterion']['quantile'],\n",
    "    # loss=args['criterion']['pearson'],\n",
    "    log_interval=args['log_interval'],  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    reduce_on_plateau_patience=args['reduce_on_plateau_patience'], # reduce learning rate if no improvement in validation loss after x epochs\n",
    ")\n",
    "tft_model.load_state_dict(torch.load(os.path.join(DIR_TRAINED, 'tft_baseline_426.pth')))\n",
    "tft_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X(1 2 3 4 5) -> Yhat(6 7 8)\n",
    "# X(2 3 4 5 6) -> Yhat(7 8 9)\n",
    "# Metric = f(Yhat(6 7 8), Y(6 7 8))\n",
    "\n",
    "test_results = trainer.test(tft_model, dataloaders=test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52c4a99fb36d68752ce25c6541fc636e9171dab977cfe863248a143161a3b436"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('11785_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
