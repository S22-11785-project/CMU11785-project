{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try DeepAR\n",
    "* PytorchForecasting [Get Started](https://pytorch-forecasting.readthedocs.io/en/stable/getting-started.html)\n",
    "* DeepAR [doc](https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.deepar.DeepAR.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_forecasting import TimeSeriesDataSet, DeepAR\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Used Yujie's cleaned version\n",
    "DIR_BYID = '/media/user/12TB1/HanLi/GitHub/CMU11785-project/local_data/content/databyid'\n",
    "\n",
    "ls_all_invest_ids = sorted([int(fn.split('.')[0]) for fn in os.listdir(os.path.join(DIR_BYID, 'target'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cols = [f\"f_{i}\" for i in range(300)]\n",
    "# Read a subset for testing\n",
    "n = 3\n",
    "ls_dfs = []\n",
    "for id in ls_all_invest_ids[:n]:\n",
    "# for id in ls_all_invest_ids:\n",
    "    df_f_id = pd.DataFrame(np.load(os.path.join(DIR_BYID, f'feats/{id}.npy')), columns=f_cols)\n",
    "    df_t_id = pd.DataFrame(np.load(os.path.join(DIR_BYID, f'target/{id}.npy')), columns=['target'])\n",
    "    df_f_id['investment_id'] = id\n",
    "    df_f_id = df_f_id[['investment_id'] + f_cols] # reorder columns\n",
    "    ls_dfs.append(pd.concat([df_t_id, df_f_id], axis=1))\n",
    "\n",
    "df = pd.concat(ls_dfs).reset_index().rename(columns={'index': 'time_id'})\n",
    "df = df.sort_values(by=['time_id']) # sort by time before splitting\n",
    "\n",
    "# # Use a few features for testing\n",
    "# df = df.iloc[:, :6]\n",
    "# f_cols = [c for c in df.columns if 'f_' in c]\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.1, shuffle=False)\n",
    "df_train, df_val = train_test_split(df_train, test_size=2/9, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset and dataloaders\n",
    "* Ref: https://pytorch-forecasting.readthedocs.io/en/stable/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import NormalDistributionLoss, QuantileLoss\n",
    "\n",
    "# define dataset\n",
    "max_encoder_length = 6\n",
    "max_prediction_length = 1\n",
    "\n",
    "# create validation and training dataset\n",
    "batch_size = 128\n",
    "max_prediction_length = 3 # prediction horizon\n",
    "max_encoder_length = 24   # lookback steps\n",
    "\n",
    "# create the dataset from the pandas dataframe\n",
    "train_dataset = TimeSeriesDataSet(\n",
    "    df_train,\n",
    "    group_ids=[\"investment_id\"],\n",
    "    target=\"target\",\n",
    "    time_idx=\"time_id\",\n",
    "    min_encoder_length=max_encoder_length // 2,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    # static_reals=[],\n",
    "    time_varying_known_reals=f_cols,\n",
    "    time_varying_unknown_reals=['target'], # Need this for DeepAR\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"investment_id\"], \n",
    "        # transformation=\"softplus\" # NOTE: do not use softplus or relu for encoder normalization with DeepAR\n",
    "    ),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "val_dataset = TimeSeriesDataSet.from_dataset(train_dataset, df_val, predict=True, stop_randomization=True)\n",
    "\n",
    "# create dataloaders for model\n",
    "train_dataloader = train_dataset.to_dataloader(train=True, batch_size=batch_size, num_workers=32)\n",
    "val_dataloader = val_dataset.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sizes of x =\n",
      "\tencoder_cat = torch.Size([128, 24, 0])\n",
      "\tencoder_cont = torch.Size([128, 24, 305])\n",
      "\tencoder_target = torch.Size([128, 24])\n",
      "\tencoder_lengths = torch.Size([128])\n",
      "\tdecoder_cat = torch.Size([128, 3, 0])\n",
      "\tdecoder_cont = torch.Size([128, 3, 305])\n",
      "\tdecoder_target = torch.Size([128, 3])\n",
      "\tdecoder_lengths = torch.Size([128])\n",
      "\tdecoder_time_idx = torch.Size([128, 3])\n",
      "\tgroups = torch.Size([128, 1])\n",
      "\ttarget_scale = torch.Size([128, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x, y = next(iter(train_dataloader))\n",
    "# print(\"x =\", x)\n",
    "# print(\"\\ny =\", y)\n",
    "print(\"\\nsizes of x =\")\n",
    "for key, value in x.items():\n",
    "    print(f\"\\t{key} = {value.size()}\")\n",
    "\n",
    "# sizes of x =\n",
    "# \tencoder_cat = torch.Size([64, 24, 0])    # (B, encoder_len, in_categorical_feats)\n",
    "# \tencoder_cont = torch.Size([64, 24, 304]) # (B, encoder_len, in_continuous_feats)\n",
    "# \tencoder_target = torch.Size([64, 24])    # (B, encoder_len)\n",
    "# \tencoder_lengths = torch.Size([64])       # (B, )\n",
    "# \tdecoder_cat = torch.Size([64, 3, 0])     # (B, decoder_len, out_categorical_feats) \n",
    "# \tdecoder_cont = torch.Size([64, 3, 304])  # (B, decoder_len, out_continuous_feats)\n",
    "# \tdecoder_target = torch.Size([64, 3])     # (B, decoder_len)\n",
    "# \tdecoder_lengths = torch.Size([64])       # (B, )\n",
    "# \tdecoder_time_idx = torch.Size([64, 3])   # (B, decoder_len)\n",
    "# \tgroups = torch.Size([64, 1])             # (B, n_investment_id)\n",
    "# \ttarget_scale = torch.Size([64, 2])       # (B, )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure DeepAR model (Han 2022-4-21 update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 128.4k\n"
     ]
    }
   ],
   "source": [
    "# configure network and trainer\n",
    "# pl.seed_everything(42)\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "model = DeepAR.from_dataset(\n",
    "    train_dataset,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=64,  # most important hyperparameter apart from learning rate\n",
    "    dropout=0.1,  # between 0.1 and 0.3 are good values\n",
    "    loss=NormalDistributionLoss(),\n",
    "    # # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {model.size()/1e3:.1f}k\")\n",
    "\n",
    "\n",
    "# find optimal learning rate\n",
    "res = trainer.tuner.lr_find(\n",
    "    model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model (Han 2022-4-21 update)\n",
    "\n",
    "* Note: use tensorboard to check the logs: run ```tensorboard --logdir=<logging_folder>```\n",
    "* To visualize tensorboard in Jupyter Notebook: \n",
    "    ```\n",
    "    %reload_ext tensorboard\n",
    "    %tensorboard --logdir=<logging_folder>\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name                   | Type                   | Params\n",
      "------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0     \n",
      "1 | logging_metrics        | ModuleList             | 0     \n",
      "2 | embeddings             | MultiEmbedding         | 0     \n",
      "3 | rnn                    | LSTM                   | 1.6 M \n",
      "4 | distribution_projector | Linear                 | 514   \n",
      "------------------------------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 M     Total params\n",
      "6.519     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 1629.7k\n",
      "Epoch 4: 100%|██████████| 21/21 [00:29<00:00,  1.40s/it, loss=79.7, v_num=p06p, train_loss_step=1.340, val_loss=0.931, train_loss_epoch=79.70]  \n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "DIR_LOGS = '/media/user/12TB1/HanLi/GitHub/CMU11785-project/logs' # Change this!\n",
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_SMAPE', \n",
    "    dirpath='/media/user/12TB1/HanLi/GitHub/CMU11785-project/logs/model_checkpoints/', \n",
    "    save_top_k=2, \n",
    "    filename='500-default-{epoch:02d}-{val_SMAPE:.2f}'\n",
    ")\n",
    "\n",
    "logger = WandbLogger(\n",
    "    entity=\"11785_project\",\n",
    "    project=\"project_runs\",\n",
    "    name='DeepAR_first_run',\n",
    "    log_model=True\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    gpus=1,\n",
    "    weights_summary=\"top\",\n",
    "    gradient_clip_val=0.1,\n",
    "    # limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    # callbacks=[lr_logger, early_stop_callback, checkpoint_callback],\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "model = DeepAR.from_dataset(\n",
    "    train_dataset,\n",
    "    cell_type='LSTM',\n",
    "    rnn_layers=3,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=256,  # most important hyperparameter apart from learning rate\n",
    "    dropout=0.1,  # between 0.1 and 0.3 are good values\n",
    "    # loss=NormalDistributionLoss(),\n",
    "    # # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {model.size()/1e3:.1f}k\")\n",
    "\n",
    "# fit network\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: test model and calculate performance metrics on test data"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52c4a99fb36d68752ce25c6541fc636e9171dab977cfe863248a143161a3b436"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('11785_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
