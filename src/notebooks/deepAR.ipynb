{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "\n",
    "# Used Yujie's cleaned version\n",
    "DIR_BYID = '/media/user/12TB1/HanLi/GitHub/CMU11785-project/local_data/content/databyid'\n",
    "\n",
    "ls_all_invest_ids = sorted([int(fn.split('.')[0]) for fn in os.listdir(os.path.join(DIR_BYID, 'target'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cols = [f\"f_{i}\" for i in range(300)]\n",
    "\n",
    "# Read a subset for testing\n",
    "n = 5\n",
    "ls_dfs = []\n",
    "for id in ls_all_invest_ids[:n]:\n",
    "    df_f_id = pd.DataFrame(np.load(os.path.join(DIR_BYID, f'feats/{id}.npy')), columns=f_cols)\n",
    "    df_t_id = pd.DataFrame(np.load(os.path.join(DIR_BYID, f'target/{id}.npy')), columns=['target'])\n",
    "    df_f_id['investment_id'] = id\n",
    "    ls_dfs.append(pd.concat([df_t_id, df_f_id], axis=1))\n",
    "\n",
    "df = pd.concat(ls_dfs).reset_index().rename(columns={'index': 'time_id'})\n",
    "df = df.sort_values(by=['time_id']) # sort by time before splitting\n",
    "df_train, df_test = train_test_split(df, test_size=0.1, shuffle=False)\n",
    "df_train, df_val = train_test_split(df_train, test_size=2/9, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>target</th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>...</th>\n",
       "      <th>f_291</th>\n",
       "      <th>f_292</th>\n",
       "      <th>f_293</th>\n",
       "      <th>f_294</th>\n",
       "      <th>f_295</th>\n",
       "      <th>f_296</th>\n",
       "      <th>f_297</th>\n",
       "      <th>f_298</th>\n",
       "      <th>f_299</th>\n",
       "      <th>investment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4880</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.810802</td>\n",
       "      <td>-0.514115</td>\n",
       "      <td>0.742368</td>\n",
       "      <td>-0.616673</td>\n",
       "      <td>-0.194255</td>\n",
       "      <td>1.771210</td>\n",
       "      <td>1.428127</td>\n",
       "      <td>1.134144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.734579</td>\n",
       "      <td>0.819155</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.387617</td>\n",
       "      <td>-1.087009</td>\n",
       "      <td>-0.929529</td>\n",
       "      <td>-0.974060</td>\n",
       "      <td>-0.343624</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.231040</td>\n",
       "      <td>0.810802</td>\n",
       "      <td>-0.514115</td>\n",
       "      <td>0.742368</td>\n",
       "      <td>-0.616673</td>\n",
       "      <td>-0.194255</td>\n",
       "      <td>1.771210</td>\n",
       "      <td>1.428127</td>\n",
       "      <td>1.134144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.734579</td>\n",
       "      <td>0.819155</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.387617</td>\n",
       "      <td>-1.087009</td>\n",
       "      <td>-0.929529</td>\n",
       "      <td>-0.974060</td>\n",
       "      <td>-0.343624</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3660</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.810802</td>\n",
       "      <td>-0.514115</td>\n",
       "      <td>0.742368</td>\n",
       "      <td>-0.616673</td>\n",
       "      <td>-0.194255</td>\n",
       "      <td>1.771210</td>\n",
       "      <td>1.428127</td>\n",
       "      <td>1.134144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.734579</td>\n",
       "      <td>0.819155</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.387617</td>\n",
       "      <td>-1.087009</td>\n",
       "      <td>-0.929529</td>\n",
       "      <td>-0.974060</td>\n",
       "      <td>-0.343624</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.300875</td>\n",
       "      <td>0.932573</td>\n",
       "      <td>0.113691</td>\n",
       "      <td>-0.402206</td>\n",
       "      <td>0.378386</td>\n",
       "      <td>-0.203938</td>\n",
       "      <td>-0.413469</td>\n",
       "      <td>0.965623</td>\n",
       "      <td>1.230508</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.095620</td>\n",
       "      <td>0.200075</td>\n",
       "      <td>0.819155</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.086764</td>\n",
       "      <td>-1.087009</td>\n",
       "      <td>-1.044826</td>\n",
       "      <td>-0.287605</td>\n",
       "      <td>0.321566</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>853</td>\n",
       "      <td>0.625438</td>\n",
       "      <td>0.125338</td>\n",
       "      <td>-0.784968</td>\n",
       "      <td>-0.026920</td>\n",
       "      <td>-0.332789</td>\n",
       "      <td>0.159759</td>\n",
       "      <td>-1.738487</td>\n",
       "      <td>-0.933227</td>\n",
       "      <td>-0.409914</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.441070</td>\n",
       "      <td>-1.074331</td>\n",
       "      <td>-1.709144</td>\n",
       "      <td>0.333215</td>\n",
       "      <td>0.256185</td>\n",
       "      <td>-1.451273</td>\n",
       "      <td>1.051513</td>\n",
       "      <td>1.932738</td>\n",
       "      <td>0.274242</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4513</th>\n",
       "      <td>853</td>\n",
       "      <td>0.007398</td>\n",
       "      <td>-3.592502</td>\n",
       "      <td>0.188260</td>\n",
       "      <td>-1.133362</td>\n",
       "      <td>-0.329699</td>\n",
       "      <td>0.431882</td>\n",
       "      <td>1.449003</td>\n",
       "      <td>1.444075</td>\n",
       "      <td>-1.126494</td>\n",
       "      <td>...</td>\n",
       "      <td>1.361796</td>\n",
       "      <td>-0.459043</td>\n",
       "      <td>0.860634</td>\n",
       "      <td>-0.921559</td>\n",
       "      <td>-0.009948</td>\n",
       "      <td>2.085927</td>\n",
       "      <td>-0.519973</td>\n",
       "      <td>-2.040276</td>\n",
       "      <td>0.799998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td>853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125338</td>\n",
       "      <td>-0.784968</td>\n",
       "      <td>-0.026920</td>\n",
       "      <td>-0.332789</td>\n",
       "      <td>0.159759</td>\n",
       "      <td>-1.738487</td>\n",
       "      <td>-0.933227</td>\n",
       "      <td>-0.409914</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.441070</td>\n",
       "      <td>-1.074331</td>\n",
       "      <td>-1.709144</td>\n",
       "      <td>0.333215</td>\n",
       "      <td>0.256185</td>\n",
       "      <td>-1.451273</td>\n",
       "      <td>1.051513</td>\n",
       "      <td>1.932738</td>\n",
       "      <td>0.274242</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5733</th>\n",
       "      <td>853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.592502</td>\n",
       "      <td>0.188260</td>\n",
       "      <td>-1.133362</td>\n",
       "      <td>-0.329699</td>\n",
       "      <td>0.431882</td>\n",
       "      <td>1.449003</td>\n",
       "      <td>1.444075</td>\n",
       "      <td>-1.126494</td>\n",
       "      <td>...</td>\n",
       "      <td>1.361796</td>\n",
       "      <td>-0.459043</td>\n",
       "      <td>0.860634</td>\n",
       "      <td>-0.921559</td>\n",
       "      <td>-0.009948</td>\n",
       "      <td>2.085927</td>\n",
       "      <td>-0.519973</td>\n",
       "      <td>-2.040276</td>\n",
       "      <td>0.799998</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3293</th>\n",
       "      <td>853</td>\n",
       "      <td>-1.222850</td>\n",
       "      <td>0.406415</td>\n",
       "      <td>0.427107</td>\n",
       "      <td>0.388093</td>\n",
       "      <td>-0.631475</td>\n",
       "      <td>-0.223392</td>\n",
       "      <td>-0.995415</td>\n",
       "      <td>0.352018</td>\n",
       "      <td>1.150299</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.402418</td>\n",
       "      <td>-0.084839</td>\n",
       "      <td>1.354303</td>\n",
       "      <td>-1.412276</td>\n",
       "      <td>-0.598099</td>\n",
       "      <td>-2.042076</td>\n",
       "      <td>0.363272</td>\n",
       "      <td>-0.971272</td>\n",
       "      <td>-0.720224</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4270 rows × 303 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      time_id    target       f_0       f_1       f_2       f_3       f_4  \\\n",
       "0           0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4880        0  0.000000  0.810802 -0.514115  0.742368 -0.616673 -0.194255   \n",
       "2440        0 -0.231040  0.810802 -0.514115  0.742368 -0.616673 -0.194255   \n",
       "3660        0  0.000000  0.810802 -0.514115  0.742368 -0.616673 -0.194255   \n",
       "1220        0 -0.300875  0.932573  0.113691 -0.402206  0.378386 -0.203938   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "853       853  0.625438  0.125338 -0.784968 -0.026920 -0.332789  0.159759   \n",
       "4513      853  0.007398 -3.592502  0.188260 -1.133362 -0.329699  0.431882   \n",
       "2073      853  0.000000  0.125338 -0.784968 -0.026920 -0.332789  0.159759   \n",
       "5733      853  0.000000 -3.592502  0.188260 -1.133362 -0.329699  0.431882   \n",
       "3293      853 -1.222850  0.406415  0.427107  0.388093 -0.631475 -0.223392   \n",
       "\n",
       "           f_5       f_6       f_7  ...     f_291     f_292     f_293  \\\n",
       "0     0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "4880  1.771210  1.428127  1.134144  ...  0.912726 -0.734579  0.819155   \n",
       "2440  1.771210  1.428127  1.134144  ...  0.912726 -0.734579  0.819155   \n",
       "3660  1.771210  1.428127  1.134144  ...  0.912726 -0.734579  0.819155   \n",
       "1220 -0.413469  0.965623  1.230508  ... -1.095620  0.200075  0.819155   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "853  -1.738487 -0.933227 -0.409914  ... -0.441070 -1.074331 -1.709144   \n",
       "4513  1.449003  1.444075 -1.126494  ...  1.361796 -0.459043  0.860634   \n",
       "2073 -1.738487 -0.933227 -0.409914  ... -0.441070 -1.074331 -1.709144   \n",
       "5733  1.449003  1.444075 -1.126494  ...  1.361796 -0.459043  0.860634   \n",
       "3293 -0.995415  0.352018  1.150299  ... -0.402418 -0.084839  1.354303   \n",
       "\n",
       "         f_294     f_295     f_296     f_297     f_298     f_299  \\\n",
       "0     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4880  0.941183 -0.387617 -1.087009 -0.929529 -0.974060 -0.343624   \n",
       "2440  0.941183 -0.387617 -1.087009 -0.929529 -0.974060 -0.343624   \n",
       "3660  0.941183 -0.387617 -1.087009 -0.929529 -0.974060 -0.343624   \n",
       "1220  0.941183 -0.086764 -1.087009 -1.044826 -0.287605  0.321566   \n",
       "...        ...       ...       ...       ...       ...       ...   \n",
       "853   0.333215  0.256185 -1.451273  1.051513  1.932738  0.274242   \n",
       "4513 -0.921559 -0.009948  2.085927 -0.519973 -2.040276  0.799998   \n",
       "2073  0.333215  0.256185 -1.451273  1.051513  1.932738  0.274242   \n",
       "5733 -0.921559 -0.009948  2.085927 -0.519973 -2.040276  0.799998   \n",
       "3293 -1.412276 -0.598099 -2.042076  0.363272 -0.971272 -0.720224   \n",
       "\n",
       "      investment_id  \n",
       "0                 0  \n",
       "4880              4  \n",
       "2440              2  \n",
       "3660              3  \n",
       "1220              1  \n",
       "...             ...  \n",
       "853               0  \n",
       "4513              3  \n",
       "2073              1  \n",
       "5733              4  \n",
       "3293              2  \n",
       "\n",
       "[4270 rows x 303 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/data/encoders.py:721: UserWarning: scale is below 1e-7 - consider not centering the data or using data with higher variance for numerical stability\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# val_dataset = TimeSeriesDataSet(\n",
    "#     df_val,\n",
    "#     group_ids=[\"investment_id\"],\n",
    "#     target=\"target\",\n",
    "#     time_idx=\"time_id\",\n",
    "#     min_encoder_length=5,\n",
    "#     max_encoder_length=5,\n",
    "#     min_prediction_length=2,\n",
    "#     max_prediction_length=2,\n",
    "#     time_varying_unknown_reals=[f\"f_{i}\" for i in range(300)],\n",
    "# )\n",
    "# test_dataset = TimeSeriesDataSet(\n",
    "#     df_test,\n",
    "#     group_ids=[\"investment_id\"],\n",
    "#     target=\"target\",\n",
    "#     time_idx=\"time_id\",\n",
    "#     min_encoder_length=5,\n",
    "#     max_encoder_length=5,\n",
    "#     min_prediction_length=2,\n",
    "#     max_prediction_length=2,\n",
    "#     time_varying_unknown_reals=[f\"f_{i}\" for i in range(300)],\n",
    "# )\n",
    "\n",
    "# # convert the dataset to a dataloader\n",
    "# dataloader = train_dataset.to_dataloader(batch_size=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try TemporalFusionTransformer\n",
    "\n",
    "* https://towardsdatascience.com/temporal-fusion-transformer-a-primer-on-deep-forecasting-in-python-4eb37f3f3594"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/data/encoders.py:721: UserWarning: scale is below 1e-7 - consider not centering the data or using data with higher variance for numerical stability\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "\n",
    "# load data\n",
    "# define dataset\n",
    "max_encoder_length = 6\n",
    "max_prediction_length = 1\n",
    "training_cutoff = \"YYYY-MM-DD\"  # day for cutoff\n",
    "\n",
    "# create validation and training dataset\n",
    "batch_size = 64\n",
    "max_prediction_length = 3\n",
    "max_encoder_length = 24\n",
    "\n",
    "# create the dataset from the pandas dataframe\n",
    "train_dataset = TimeSeriesDataSet(\n",
    "    df_train,\n",
    "    group_ids=[\"investment_id\"],\n",
    "    target=\"target\",\n",
    "    time_idx=\"time_id\",\n",
    "    min_encoder_length=max_encoder_length // 2,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    # static_reals=[],\n",
    "    time_varying_unknown_reals=['target'] + [f\"f_{i}\" for i in range(300)],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"investment_id\"], transformation=\"softplus\"\n",
    "    ),  # use softplus and normalize by group\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "val_dataset = TimeSeriesDataSet.from_dataset(train_dataset, df_train, predict=True, stop_randomization=True)\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 64  # set this between 32 to 128\n",
    "train_dataloader = train_dataset.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = val_dataset.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimal learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 226.9k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best initial lr:   1%|          | 1/100 [02:19<3:49:29, 139.08s/it]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Finding best initial lr: 100%|██████████| 100/100 [01:19<00:00,  1.06it/s]Restoring states from the checkpoint path at /media/user/12TB1/HanLi/GitHub/CMU11785-project/src/.lr_find_4dce4619-5c87-406e-804a-43bf449b9113.ckpt\n",
      "/home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1721: UserWarning: Be aware that when using `ckpt_path`, callbacks used to create the checkpoint need to be provided during `Trainer` instantiation. Please add the following callbacks: [\"ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None, 'save_on_train_epoch_end': None}\"].\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr: 100%|██████████| 100/100 [01:22<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suggested learning rate: 1.0471285480508992\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# configure network and trainer\n",
    "pl.seed_everything(42)\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    train_dataset,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=8,  # set to <= hidden_size\n",
    "    output_size=7,  # 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "\n",
    "# find optimal learning rate\n",
    "res = trainer.tuner.lr_find(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try DeepAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Encoder and decoder variables have to be the same apart from target variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000010vscode-remote?line=20'>21</a>\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000010vscode-remote?line=21'>22</a>\u001b[0m     max_epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000010vscode-remote?line=22'>23</a>\u001b[0m     gpus\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000010vscode-remote?line=25'>26</a>\u001b[0m     callbacks\u001b[39m=\u001b[39m[lr_logger, early_stop_callback],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000010vscode-remote?line=26'>27</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000010vscode-remote?line=28'>29</a>\u001b[0m \u001b[39m# create the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000010vscode-remote?line=29'>30</a>\u001b[0m model \u001b[39m=\u001b[39m DeepAR\u001b[39m.\u001b[39;49mfrom_dataset(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000010vscode-remote?line=30'>31</a>\u001b[0m     train_dataset,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000010vscode-remote?line=31'>32</a>\u001b[0m     allowed_encoder_known_variable_names\u001b[39m=\u001b[39;49mf_cols\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000010vscode-remote?line=32'>33</a>\u001b[0m     \u001b[39m# learning_rate=1e-3,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000010vscode-remote?line=33'>34</a>\u001b[0m     \u001b[39m# hidden_size=32,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000010vscode-remote?line=34'>35</a>\u001b[0m     \u001b[39m# # attention_head_size=1,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000010vscode-remote?line=35'>36</a>\u001b[0m     \u001b[39m# dropout=0.1,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000010vscode-remote?line=36'>37</a>\u001b[0m     \u001b[39m# # hidden_continuous_size=16,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000010vscode-remote?line=37'>38</a>\u001b[0m     \u001b[39m# # output_size=1,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000010vscode-remote?line=38'>39</a>\u001b[0m     \u001b[39m# loss=QuantileLoss(),\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000010vscode-remote?line=39'>40</a>\u001b[0m     \u001b[39m# log_interval=2,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000010vscode-remote?line=40'>41</a>\u001b[0m     \u001b[39m# reduce_on_plateau_patience=4\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000010vscode-remote?line=41'>42</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000010vscode-remote?line=42'>43</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of parameters in network: \u001b[39m\u001b[39m{\u001b[39;00mtft\u001b[39m.\u001b[39msize()\u001b[39m/\u001b[39m\u001b[39m1e3\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.1f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39mk\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000010vscode-remote?line=44'>45</a>\u001b[0m \u001b[39m# find optimal learning rate (set limit_train_batches to 1.0 and log_interval = -1)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py:187\u001b[0m, in \u001b[0;36mDeepAR.from_dataset\u001b[0;34m(cls, dataset, allowed_encoder_known_variable_names, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=181'>182</a>\u001b[0m new_kwargs\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=182'>183</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(dataset\u001b[39m.\u001b[39mtarget_normalizer, NaNLabelEncoder) \u001b[39mand\u001b[39;00m (\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=183'>184</a>\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(dataset\u001b[39m.\u001b[39mtarget_normalizer, MultiNormalizer)\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=184'>185</a>\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mall\u001b[39m([\u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(normalizer, NaNLabelEncoder) \u001b[39mfor\u001b[39;00m normalizer \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39mtarget_normalizer])\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=185'>186</a>\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39mtarget(s) should be continuous - categorical targets are not supported\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# todo: remove this restriction\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=186'>187</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfrom_dataset(\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=187'>188</a>\u001b[0m     dataset, allowed_encoder_known_variable_names\u001b[39m=\u001b[39;49mallowed_encoder_known_variable_names, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_kwargs\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=188'>189</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py:1474\u001b[0m, in \u001b[0;36mBaseModelWithCovariates.from_dataset\u001b[0;34m(cls, dataset, allowed_encoder_known_variable_names, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py?line=1453'>1454</a>\u001b[0m new_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m   <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py?line=1454'>1455</a>\u001b[0m     static_categoricals\u001b[39m=\u001b[39mdataset\u001b[39m.\u001b[39mstatic_categoricals,\n\u001b[1;32m   <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py?line=1455'>1456</a>\u001b[0m     time_varying_categoricals_encoder\u001b[39m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py?line=1470'>1471</a>\u001b[0m     categorical_groups\u001b[39m=\u001b[39mdataset\u001b[39m.\u001b[39mvariable_groups,\n\u001b[1;32m   <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py?line=1471'>1472</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py?line=1472'>1473</a>\u001b[0m new_kwargs\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[0;32m-> <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py?line=1473'>1474</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfrom_dataset(dataset, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py:1797\u001b[0m, in \u001b[0;36mAutoRegressiveBaseModel.from_dataset\u001b[0;34m(cls, dataset, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py?line=1793'>1794</a>\u001b[0m     \u001b[39massert\u001b[39;00m lag \u001b[39m==\u001b[39m \u001b[39mset\u001b[39m(lags\u001b[39m.\u001b[39mget(target, [])), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mall target lags in dataset must be the same but found \u001b[39m\u001b[39m{\u001b[39;00mlags\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py?line=1795'>1796</a>\u001b[0m kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mtarget_lags\u001b[39m\u001b[39m\"\u001b[39m, {name: dataset\u001b[39m.\u001b[39m_get_lagged_names(name) \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m lags})\n\u001b[0;32m-> <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py?line=1796'>1797</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfrom_dataset(dataset, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py:987\u001b[0m, in \u001b[0;36mBaseModel.from_dataset\u001b[0;34m(cls, dataset, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py?line=984'>985</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39moutput_transformer\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py?line=985'>986</a>\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39moutput_transformer\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mtarget_normalizer\n\u001b[0;32m--> <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py?line=986'>987</a>\u001b[0m net \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py?line=987'>988</a>\u001b[0m net\u001b[39m.\u001b[39mdataset_parameters \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mget_parameters()\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/base_model.py?line=988'>989</a>\u001b[0m \u001b[39mif\u001b[39;00m dataset\u001b[39m.\u001b[39mmulti_target:\n",
      "File \u001b[0;32m~/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py:130\u001b[0m, in \u001b[0;36mDeepAR.__init__\u001b[0;34m(self, cell_type, hidden_size, rnn_layers, dropout, static_categoricals, static_reals, time_varying_categoricals_encoder, time_varying_categoricals_decoder, categorical_groups, time_varying_reals_encoder, time_varying_reals_decoder, embedding_sizes, embedding_paddings, embedding_labels, x_reals, x_categoricals, n_validation_samples, n_plotting_samples, target, target_lags, loss, logging_metrics, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=121'>122</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings \u001b[39m=\u001b[39m MultiEmbedding(\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=122'>123</a>\u001b[0m     embedding_sizes\u001b[39m=\u001b[39membedding_sizes,\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=123'>124</a>\u001b[0m     embedding_paddings\u001b[39m=\u001b[39membedding_paddings,\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=124'>125</a>\u001b[0m     categorical_groups\u001b[39m=\u001b[39mcategorical_groups,\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=125'>126</a>\u001b[0m     x_categoricals\u001b[39m=\u001b[39mx_categoricals,\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=126'>127</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=128'>129</a>\u001b[0m lagged_target_names \u001b[39m=\u001b[39m [l \u001b[39mfor\u001b[39;00m lags \u001b[39min\u001b[39;00m target_lags\u001b[39m.\u001b[39mvalues() \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lags]\n\u001b[0;32m--> <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=129'>130</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_variables) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(to_list(target)) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(lagged_target_names) \u001b[39m==\u001b[39m \u001b[39mset\u001b[39m(\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=130'>131</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder_variables\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=131'>132</a>\u001b[0m ) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(lagged_target_names), \u001b[39m\"\u001b[39m\u001b[39mEncoder and decoder variables have to be the same apart from target variable\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=132'>133</a>\u001b[0m \u001b[39mfor\u001b[39;00m targeti \u001b[39min\u001b[39;00m to_list(target):\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=133'>134</a>\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=134'>135</a>\u001b[0m         targeti \u001b[39min\u001b[39;00m time_varying_reals_encoder\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=135'>136</a>\u001b[0m     ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtarget \u001b[39m\u001b[39m{\u001b[39;00mtargeti\u001b[39m}\u001b[39;00m\u001b[39m has to be real\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# todo: remove this restriction\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Encoder and decoder variables have to be the same apart from target variable"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "\n",
    "from pytorch_forecasting import TimeSeriesDataSet, DeepAR\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "\n",
    "# load data\n",
    "# define dataset\n",
    "max_encoder_length = 6\n",
    "max_prediction_length = 1\n",
    "training_cutoff = \"YYYY-MM-DD\"  # day for cutoff\n",
    "\n",
    "# create validation and training dataset\n",
    "batch_size = 64\n",
    "train_dataloader = train_dataset.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\n",
    "val_dataloader = val_dataset.to_dataloader(train=False, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "# define trainer with early stopping\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=1, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    gpus=1,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    ")\n",
    "\n",
    "# create the model\n",
    "model = DeepAR.from_dataset(\n",
    "    train_dataset,\n",
    "    allowed_encoder_known_variable_names=f_cols\n",
    "    # learning_rate=1e-3,\n",
    "    # hidden_size=32,\n",
    "    # # attention_head_size=1,\n",
    "    # dropout=0.1,\n",
    "    # # hidden_continuous_size=16,\n",
    "    # # output_size=1,\n",
    "    # loss=QuantileLoss(),\n",
    "    # log_interval=2,\n",
    "    # reduce_on_plateau_patience=4\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "# find optimal learning rate (set limit_train_batches to 1.0 and log_interval = -1)\n",
    "res = trainer.tuner.lr_find(\n",
    "    model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader, early_stop_threshold=100, max_lr=0.3,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()\n",
    "\n",
    "# # fit the model\n",
    "# trainer.fit(\n",
    "#     model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>target</th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>...</th>\n",
       "      <th>f_291</th>\n",
       "      <th>f_292</th>\n",
       "      <th>f_293</th>\n",
       "      <th>f_294</th>\n",
       "      <th>f_295</th>\n",
       "      <th>f_296</th>\n",
       "      <th>f_297</th>\n",
       "      <th>f_298</th>\n",
       "      <th>f_299</th>\n",
       "      <th>investment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4880</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.810802</td>\n",
       "      <td>-0.514115</td>\n",
       "      <td>0.742368</td>\n",
       "      <td>-0.616673</td>\n",
       "      <td>-0.194255</td>\n",
       "      <td>1.771210</td>\n",
       "      <td>1.428127</td>\n",
       "      <td>1.134144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.734579</td>\n",
       "      <td>0.819155</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.387617</td>\n",
       "      <td>-1.087009</td>\n",
       "      <td>-0.929529</td>\n",
       "      <td>-0.974060</td>\n",
       "      <td>-0.343624</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.231040</td>\n",
       "      <td>0.810802</td>\n",
       "      <td>-0.514115</td>\n",
       "      <td>0.742368</td>\n",
       "      <td>-0.616673</td>\n",
       "      <td>-0.194255</td>\n",
       "      <td>1.771210</td>\n",
       "      <td>1.428127</td>\n",
       "      <td>1.134144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.734579</td>\n",
       "      <td>0.819155</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.387617</td>\n",
       "      <td>-1.087009</td>\n",
       "      <td>-0.929529</td>\n",
       "      <td>-0.974060</td>\n",
       "      <td>-0.343624</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3660</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.810802</td>\n",
       "      <td>-0.514115</td>\n",
       "      <td>0.742368</td>\n",
       "      <td>-0.616673</td>\n",
       "      <td>-0.194255</td>\n",
       "      <td>1.771210</td>\n",
       "      <td>1.428127</td>\n",
       "      <td>1.134144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912726</td>\n",
       "      <td>-0.734579</td>\n",
       "      <td>0.819155</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.387617</td>\n",
       "      <td>-1.087009</td>\n",
       "      <td>-0.929529</td>\n",
       "      <td>-0.974060</td>\n",
       "      <td>-0.343624</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.300875</td>\n",
       "      <td>0.932573</td>\n",
       "      <td>0.113691</td>\n",
       "      <td>-0.402206</td>\n",
       "      <td>0.378386</td>\n",
       "      <td>-0.203938</td>\n",
       "      <td>-0.413469</td>\n",
       "      <td>0.965623</td>\n",
       "      <td>1.230508</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.095620</td>\n",
       "      <td>0.200075</td>\n",
       "      <td>0.819155</td>\n",
       "      <td>0.941183</td>\n",
       "      <td>-0.086764</td>\n",
       "      <td>-1.087009</td>\n",
       "      <td>-1.044826</td>\n",
       "      <td>-0.287605</td>\n",
       "      <td>0.321566</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>853</td>\n",
       "      <td>0.625438</td>\n",
       "      <td>0.125338</td>\n",
       "      <td>-0.784968</td>\n",
       "      <td>-0.026920</td>\n",
       "      <td>-0.332789</td>\n",
       "      <td>0.159759</td>\n",
       "      <td>-1.738487</td>\n",
       "      <td>-0.933227</td>\n",
       "      <td>-0.409914</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.441070</td>\n",
       "      <td>-1.074331</td>\n",
       "      <td>-1.709144</td>\n",
       "      <td>0.333215</td>\n",
       "      <td>0.256185</td>\n",
       "      <td>-1.451273</td>\n",
       "      <td>1.051513</td>\n",
       "      <td>1.932738</td>\n",
       "      <td>0.274242</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4513</th>\n",
       "      <td>853</td>\n",
       "      <td>0.007398</td>\n",
       "      <td>-3.592502</td>\n",
       "      <td>0.188260</td>\n",
       "      <td>-1.133362</td>\n",
       "      <td>-0.329699</td>\n",
       "      <td>0.431882</td>\n",
       "      <td>1.449003</td>\n",
       "      <td>1.444075</td>\n",
       "      <td>-1.126494</td>\n",
       "      <td>...</td>\n",
       "      <td>1.361796</td>\n",
       "      <td>-0.459043</td>\n",
       "      <td>0.860634</td>\n",
       "      <td>-0.921559</td>\n",
       "      <td>-0.009948</td>\n",
       "      <td>2.085927</td>\n",
       "      <td>-0.519973</td>\n",
       "      <td>-2.040276</td>\n",
       "      <td>0.799998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td>853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125338</td>\n",
       "      <td>-0.784968</td>\n",
       "      <td>-0.026920</td>\n",
       "      <td>-0.332789</td>\n",
       "      <td>0.159759</td>\n",
       "      <td>-1.738487</td>\n",
       "      <td>-0.933227</td>\n",
       "      <td>-0.409914</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.441070</td>\n",
       "      <td>-1.074331</td>\n",
       "      <td>-1.709144</td>\n",
       "      <td>0.333215</td>\n",
       "      <td>0.256185</td>\n",
       "      <td>-1.451273</td>\n",
       "      <td>1.051513</td>\n",
       "      <td>1.932738</td>\n",
       "      <td>0.274242</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5733</th>\n",
       "      <td>853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.592502</td>\n",
       "      <td>0.188260</td>\n",
       "      <td>-1.133362</td>\n",
       "      <td>-0.329699</td>\n",
       "      <td>0.431882</td>\n",
       "      <td>1.449003</td>\n",
       "      <td>1.444075</td>\n",
       "      <td>-1.126494</td>\n",
       "      <td>...</td>\n",
       "      <td>1.361796</td>\n",
       "      <td>-0.459043</td>\n",
       "      <td>0.860634</td>\n",
       "      <td>-0.921559</td>\n",
       "      <td>-0.009948</td>\n",
       "      <td>2.085927</td>\n",
       "      <td>-0.519973</td>\n",
       "      <td>-2.040276</td>\n",
       "      <td>0.799998</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3293</th>\n",
       "      <td>853</td>\n",
       "      <td>-1.222850</td>\n",
       "      <td>0.406415</td>\n",
       "      <td>0.427107</td>\n",
       "      <td>0.388093</td>\n",
       "      <td>-0.631475</td>\n",
       "      <td>-0.223392</td>\n",
       "      <td>-0.995415</td>\n",
       "      <td>0.352018</td>\n",
       "      <td>1.150299</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.402418</td>\n",
       "      <td>-0.084839</td>\n",
       "      <td>1.354303</td>\n",
       "      <td>-1.412276</td>\n",
       "      <td>-0.598099</td>\n",
       "      <td>-2.042076</td>\n",
       "      <td>0.363272</td>\n",
       "      <td>-0.971272</td>\n",
       "      <td>-0.720224</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4270 rows × 303 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      time_id    target       f_0       f_1       f_2       f_3       f_4  \\\n",
       "0           0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4880        0  0.000000  0.810802 -0.514115  0.742368 -0.616673 -0.194255   \n",
       "2440        0 -0.231040  0.810802 -0.514115  0.742368 -0.616673 -0.194255   \n",
       "3660        0  0.000000  0.810802 -0.514115  0.742368 -0.616673 -0.194255   \n",
       "1220        0 -0.300875  0.932573  0.113691 -0.402206  0.378386 -0.203938   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "853       853  0.625438  0.125338 -0.784968 -0.026920 -0.332789  0.159759   \n",
       "4513      853  0.007398 -3.592502  0.188260 -1.133362 -0.329699  0.431882   \n",
       "2073      853  0.000000  0.125338 -0.784968 -0.026920 -0.332789  0.159759   \n",
       "5733      853  0.000000 -3.592502  0.188260 -1.133362 -0.329699  0.431882   \n",
       "3293      853 -1.222850  0.406415  0.427107  0.388093 -0.631475 -0.223392   \n",
       "\n",
       "           f_5       f_6       f_7  ...     f_291     f_292     f_293  \\\n",
       "0     0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "4880  1.771210  1.428127  1.134144  ...  0.912726 -0.734579  0.819155   \n",
       "2440  1.771210  1.428127  1.134144  ...  0.912726 -0.734579  0.819155   \n",
       "3660  1.771210  1.428127  1.134144  ...  0.912726 -0.734579  0.819155   \n",
       "1220 -0.413469  0.965623  1.230508  ... -1.095620  0.200075  0.819155   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "853  -1.738487 -0.933227 -0.409914  ... -0.441070 -1.074331 -1.709144   \n",
       "4513  1.449003  1.444075 -1.126494  ...  1.361796 -0.459043  0.860634   \n",
       "2073 -1.738487 -0.933227 -0.409914  ... -0.441070 -1.074331 -1.709144   \n",
       "5733  1.449003  1.444075 -1.126494  ...  1.361796 -0.459043  0.860634   \n",
       "3293 -0.995415  0.352018  1.150299  ... -0.402418 -0.084839  1.354303   \n",
       "\n",
       "         f_294     f_295     f_296     f_297     f_298     f_299  \\\n",
       "0     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4880  0.941183 -0.387617 -1.087009 -0.929529 -0.974060 -0.343624   \n",
       "2440  0.941183 -0.387617 -1.087009 -0.929529 -0.974060 -0.343624   \n",
       "3660  0.941183 -0.387617 -1.087009 -0.929529 -0.974060 -0.343624   \n",
       "1220  0.941183 -0.086764 -1.087009 -1.044826 -0.287605  0.321566   \n",
       "...        ...       ...       ...       ...       ...       ...   \n",
       "853   0.333215  0.256185 -1.451273  1.051513  1.932738  0.274242   \n",
       "4513 -0.921559 -0.009948  2.085927 -0.519973 -2.040276  0.799998   \n",
       "2073  0.333215  0.256185 -1.451273  1.051513  1.932738  0.274242   \n",
       "5733 -0.921559 -0.009948  2.085927 -0.519973 -2.040276  0.799998   \n",
       "3293 -1.412276 -0.598099 -2.042076  0.363272 -0.971272 -0.720224   \n",
       "\n",
       "      investment_id  \n",
       "0                 0  \n",
       "4880              4  \n",
       "2440              2  \n",
       "3660              3  \n",
       "1220              1  \n",
       "...             ...  \n",
       "853               0  \n",
       "4513              3  \n",
       "2073              1  \n",
       "5733              4  \n",
       "3293              2  \n",
       "\n",
       "[4270 rows x 303 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:244: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:244: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "target target has to be real",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000011vscode-remote?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m DeepAR(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000011vscode-remote?line=1'>2</a>\u001b[0m     cell_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mLSTM\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000011vscode-remote?line=2'>3</a>\u001b[0m     hidden_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000011vscode-remote?line=3'>4</a>\u001b[0m     rnn_layers\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000011vscode-remote?line=4'>5</a>\u001b[0m     dropout\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000011vscode-remote?line=5'>6</a>\u001b[0m     target\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtarget\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000011vscode-remote?line=6'>7</a>\u001b[0m     time_varying_reals_encoder\u001b[39m=\u001b[39;49m[],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000011vscode-remote?line=7'>8</a>\u001b[0m     time_varying_reals_decoder\u001b[39m=\u001b[39;49m[],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000011vscode-remote?line=8'>9</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Buser@srg-gpu.dhcp.lbl.gov/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/deepAR.ipynb#ch0000011vscode-remote?line=9'>10</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py:134\u001b[0m, in \u001b[0;36mDeepAR.__init__\u001b[0;34m(self, cell_type, hidden_size, rnn_layers, dropout, static_categoricals, static_reals, time_varying_categoricals_encoder, time_varying_categoricals_decoder, categorical_groups, time_varying_reals_encoder, time_varying_reals_decoder, embedding_sizes, embedding_paddings, embedding_labels, x_reals, x_categoricals, n_validation_samples, n_plotting_samples, target, target_lags, loss, logging_metrics, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=129'>130</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_variables) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(to_list(target)) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(lagged_target_names) \u001b[39m==\u001b[39m \u001b[39mset\u001b[39m(\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=130'>131</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder_variables\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=131'>132</a>\u001b[0m ) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(lagged_target_names), \u001b[39m\"\u001b[39m\u001b[39mEncoder and decoder variables have to be the same apart from target variable\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=132'>133</a>\u001b[0m \u001b[39mfor\u001b[39;00m targeti \u001b[39min\u001b[39;00m to_list(target):\n\u001b[0;32m--> <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=133'>134</a>\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=134'>135</a>\u001b[0m         targeti \u001b[39min\u001b[39;00m time_varying_reals_encoder\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=135'>136</a>\u001b[0m     ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtarget \u001b[39m\u001b[39m{\u001b[39;00mtargeti\u001b[39m}\u001b[39;00m\u001b[39m has to be real\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# todo: remove this restriction\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=136'>137</a>\u001b[0m \u001b[39massert\u001b[39;00m (\u001b[39misinstance\u001b[39m(target, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(loss, DistributionLoss)) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=137'>138</a>\u001b[0m     \u001b[39misinstance\u001b[39m(target, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(loss, MultiLoss) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(loss) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(target)\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=138'>139</a>\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39mnumber of targets should be equivalent to number of loss metrics\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/user/anaconda3/envs/11785_project/lib/python3.9/site-packages/pytorch_forecasting/models/deepar/__init__.py?line=140'>141</a>\u001b[0m rnn_class \u001b[39m=\u001b[39m get_rnn(cell_type)\n",
      "\u001b[0;31mAssertionError\u001b[0m: target target has to be real"
     ]
    }
   ],
   "source": [
    "model = DeepAR(\n",
    "    cell_type='LSTM',\n",
    "    hidden_size=32,\n",
    "    rnn_layers=2,\n",
    "    dropout=0.1,\n",
    "    target='target',\n",
    "    time_varying_reals_encoder=[],\n",
    "    time_varying_reals_decoder=[],\n",
    "\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52c4a99fb36d68752ce25c6541fc636e9171dab977cfe863248a143161a3b436"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('11785_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
