{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample investment from full data and create PytorchForecasting TimeSeriesDataSet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DIR_DATA = '/media/user/12TB1/HanLi/GitHub/CMU11785-project/local_data'\n",
    "DIR_BYID = os.path.join(DIR_DATA, 'content/databyid')\n",
    "\n",
    "def create_dataset(\n",
    "    ls_ids,\n",
    "    dir_pf_dataset,\n",
    "    max_prediction_length=3,\n",
    "    max_encoder_length=14,\n",
    "):\n",
    "    \"\"\" Train : Val : Test = 7 : 2 : 1\n",
    "    \"\"\"\n",
    "    f_cols = [f\"f_{i}\" for i in range(300)]\n",
    "    ls_dfs = []\n",
    "    for id in ls_ids:\n",
    "        df_f_id = pd.DataFrame(np.load(os.path.join(DIR_BYID, f'feats/{id}.npy')), columns=f_cols)\n",
    "        df_t_id = pd.DataFrame(np.load(os.path.join(DIR_BYID, f'target/{id}.npy')), columns=['target'])\n",
    "        df_f_id['investment_id'] = id\n",
    "        df_id = pd.concat([df_t_id, df_f_id], axis=1)\n",
    "        df_id['investment_id'] = df_id['investment_id'].astype(str)\n",
    "        ls_dfs.append(df_id)\n",
    "\n",
    "    df = pd.concat(ls_dfs).reset_index().rename(columns={'index': 'time_id'})\n",
    "    df = df.sort_values(by=['time_id']) # sort by time before splitting\n",
    "\n",
    "    df = df.loc[df['time_id'] >= 400]\n",
    "\n",
    "    df_train, df_test = train_test_split(df, test_size=0.1, shuffle=False)\n",
    "    df_train, df_val = train_test_split(df_train, test_size=2/9, shuffle=False)\n",
    "\n",
    "    print(f\"Create and save new dataset with {len(ls_ids)} samples...\")\n",
    "    # create the dataset from the pandas dataframe\n",
    "    train_dataset = TimeSeriesDataSet(\n",
    "        df_train,\n",
    "        group_ids=[\"investment_id\"],\n",
    "        target=\"target\",\n",
    "        time_idx=\"time_id\",\n",
    "        min_encoder_length=max_encoder_length // 2,\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        min_prediction_length=1,\n",
    "        max_prediction_length=max_prediction_length,\n",
    "        static_categoricals=[\"investment_id\"],\n",
    "        static_reals=[],\n",
    "        time_varying_known_categoricals=[],\n",
    "        time_varying_known_reals=f_cols,\n",
    "        time_varying_unknown_categoricals=[],\n",
    "        time_varying_unknown_reals=['target'],\n",
    "        target_normalizer=GroupNormalizer( # normalize the targe for each investment_id along corresponding time_idx\n",
    "            groups=[\"investment_id\"], \n",
    "            transformation=None # NOTE: do not use softplus or relu for encoder normalization with DeepAR\n",
    "            # transformation=\"softplus\" # NOTE: do not use softplus or relu for encoder normalization with DeepAR\n",
    "        ),\n",
    "        # Add additional features\n",
    "        add_relative_time_idx=True,\n",
    "        add_target_scales=True,\n",
    "        add_encoder_length=True,\n",
    "    )\n",
    "    val_dataset = TimeSeriesDataSet.from_dataset(train_dataset, df_val, predict=True, stop_randomization=True)\n",
    "    test_dataset = TimeSeriesDataSet.from_dataset(train_dataset, df_test, predict=True, stop_randomization=True)\n",
    "    # Save dataset to accelerate\n",
    "    train_dataset.save(os.path.join(dir_pf_dataset, f'pf_train_{len(ls_ids)}_samples.pf'))\n",
    "    val_dataset.save(os.path.join(dir_pf_dataset, f'pf_val_{len(ls_ids)}_samples.pf'))\n",
    "    test_dataset.save(os.path.join(dir_pf_dataset, f'pf_test_{len(ls_ids)}_samples.pf'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_pf_dataset = os.path.join(DIR_DATA, 'pf_dataset_deepar')\n",
    "if not os.path.exists(dir_pf_dataset):\n",
    "    os.makedirs(dir_pf_dataset)\n",
    "\n",
    "ls_all_invest_ids = sorted([int(fn.split('.')[0]) for fn in os.listdir(os.path.join(DIR_BYID, 'target'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create and save new dataset with 10 samples...\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(11785)\n",
    "\n",
    "n_samples = [500, 1000, 1500, 2000]\n",
    "n_samples = [10]\n",
    "for n_sample in n_samples:\n",
    "    ls_subset = random.sample(ls_all_invest_ids, n_sample)\n",
    "    create_dataset(ls_subset, dir_pf_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create and save new dataset with 1000 samples...\n"
     ]
    }
   ],
   "source": [
    "ls_test_500 = pd.read_pickle('/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/data/test_500_ids.pkl')\n",
    "ls_test_1000 = pd.read_pickle('/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/data/test_1000_ids.pkl')\n",
    "ls_test_1500 = pd.read_pickle('/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/data/test_1500_ids.pkl')\n",
    "ls_test_2000 = pd.read_pickle('/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/data/test_2000_ids.pkl')\n",
    "ls_test_all = pd.read_pickle('/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/data/test_all_ids.pkl')\n",
    "\n",
    "dir_pf_dataset = os.path.join(DIR_DATA, 'pf_dataset_tft')\n",
    "if not os.path.exists(dir_pf_dataset):\n",
    "    os.makedirs(dir_pf_dataset)\n",
    "\n",
    "create_dataset(ls_test_1000, dir_pf_dataset, max_prediction_length=3, max_encoder_length=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create and save new dataset with 500 samples...\n"
     ]
    }
   ],
   "source": [
    "ls_test_500 = pd.read_pickle('/media/user/12TB1/HanLi/GitHub/CMU11785-project/src/data/test_500_ids.pkl')\n",
    "\n",
    "\n",
    "dir_pf_dataset = os.path.join(DIR_DATA, 'pf_dataset_test')\n",
    "if not os.path.exists(dir_pf_dataset):\n",
    "    os.makedirs(dir_pf_dataset)\n",
    "\n",
    "create_dataset(ls_test_500, dir_pf_dataset, max_prediction_length=3, max_encoder_length=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "train_dataset = TimeSeriesDataSet.load(os.path.join(dir_pf_dataset, f'pf_train_{n}_samples.pf'))\n",
    "val_dataset = TimeSeriesDataSet.load(os.path.join(dir_pf_dataset, f'pf_val_{n}_samples.pf'))\n",
    "test_dataset = TimeSeriesDataSet.load(os.path.join(dir_pf_dataset, f'pf_test_{n}_samples.pf'))\n",
    "\n",
    "# Create dataloader from dataset\n",
    "batch_size = 64  # set this between 32 to 128\n",
    "train_dataloader = train_dataset.to_dataloader(train=True, batch_size=batch_size, num_workers=32)\n",
    "val_dataloader = val_dataset.to_dataloader(train=False, batch_size=batch_size * 5, num_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create and save new dataset with 10 samples...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ls_subset = random.sample(ls_all_invest_ids, 10)\n",
    "ls_subset\n",
    "create_dataset(ls_subset, dir_pf_dataset)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52c4a99fb36d68752ce25c6541fc636e9171dab977cfe863248a143161a3b436"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('11785_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
