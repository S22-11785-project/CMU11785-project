{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1684169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class V_AR():\n",
    "    '''\n",
    "    Vector Autoregressions (VAR)\n",
    "    Y_t = \\sum{A_i*Y_i} + Res\n",
    "    Y: T * K\n",
    "    T: number of time steps\n",
    "    K: number of features\n",
    "\n",
    "    '''\n",
    "    def __init__(self, data):\n",
    "        '''\n",
    "        arg data: features\n",
    "            p: max lag order\n",
    "\n",
    "        '''\n",
    "        self.data = data\n",
    "        model = VAR(data)\n",
    "        self.results = model.fit()\n",
    "    \n",
    "    def residual(self):\n",
    "        '''\n",
    "        transform features to its VAR residuals\n",
    "        return: VAR residuals\n",
    "\n",
    "        '''\n",
    "        fit = self.results.fittedvalues\n",
    "        return self.data - fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a210059",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_process():\n",
    "    def __init__(self, train_1000_address, inv_list_address, good_inv_list_address):\n",
    "        inv = pd.read_pickle(inv_list_address)\n",
    "        inv_good = pd.read_pickle(good_inv_list_address)\n",
    "        self.s = [i for i in inv if i in inv_good]\n",
    "        df = pd.read_pickle(train_1000_address)\n",
    "        self.target = df['target']\n",
    "        self.feature = df.drop('target', axis=1)\n",
    "    \n",
    "    def scale():\n",
    "        scaler = MinMaxScaler()\n",
    "        m = []\n",
    "        name = []\n",
    "        for x in self.s:\n",
    "            f = self.feature.loc[x].dropna()\n",
    "            columns = ['f_{}'.format(i) for i in range(300)]\n",
    "            f_scale = pd.DataFrame(scaler.fit_transform(f), index=f.index, columns=columns)\n",
    "            m.append(f_scale)\n",
    "            name.append(x)\n",
    "        df_scale = pd.concat(m, keys=name, names=['investment_id'])\n",
    "        return df_scale.join(self.target)\n",
    "\n",
    "    def ar():\n",
    "        m = []\n",
    "        name = []\n",
    "        for x in self.s:\n",
    "            f = self.feature.loc[x].dropna()\n",
    "            V = V_AR(f)\n",
    "            m.append(V.residual())\n",
    "            name.append(x)\n",
    "        df_ar = pd.concat(m, keys=name, names=['investment_id'])\n",
    "        return df_ar.join(self.target)\n",
    "  \n",
    "    def pca():\n",
    "        m = []\n",
    "        name = []\n",
    "        for x in self.s:\n",
    "            f = self.feature.loc[x].dropna()\n",
    "            n = min(150, f.shape[0])\n",
    "            pca = PCA(n_components=n)\n",
    "            pca.fit(f)\n",
    "            evr = pca.explained_variance_ratio_\n",
    "            num = np.cumsum(evr)\n",
    "            n_comp = np.where(num>0.9)[0][0]\n",
    "            pca = PCA(n_components=n_comp)\n",
    "            columns = ['fpca_{}'.format(i) for i in range(n_comp)]\n",
    "            f_pca = pca.fit_transform(f)\n",
    "            f_pca = pd.DataFrame(f_pca, index=f.index, columns=columns)\n",
    "            m.append(f_pca)\n",
    "            name.append(x)\n",
    "        df_pca = pd.concat(m, keys=name, names=['investment_id'])\n",
    "        return df_pca.join(self.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184aba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset():\n",
    "    def __init__(df_address, inv_list_address, start_time):\n",
    "        f = pd.read_pickle(df_address)\n",
    "        inv_id = pd.read_pickle(inv_list_address)\n",
    "        iterables = [inv_id, range(start_time, 1220)]\n",
    "        index = pd.MultiIndex.from_product(iterables, names=['investment_id', 'time_id'])\n",
    "        n = len(inv_id) * len(range(start_time, 1220))\n",
    "        df = pd.DataFrame(np.empty(n), index=index)\n",
    "        df = df.join(f)\n",
    "        df = df.drop(0, axis=1)\n",
    "        df = df.fillna(0).reset_index()\n",
    "        df['investment_id'] = df['investment_id'].astype(str)\n",
    "        self.df = df.sort_values(by=['time_id'])\n",
    "        # self.df.tp_pickle(./df_246_*******.pkl)\n",
    "        \n",
    "    def data_generate():\n",
    "        df_train, df_test = train_test_split(self.df, test_size=0.1, shuffle=False)\n",
    "        df_train, df_val = train_test_split(df_train, test_size=2/9, shuffle=False)\n",
    "        max_prediction_length=3\n",
    "        max_encoder_length=14\n",
    "        train_dataset = TimeSeriesDataSet(\n",
    "            df_train,\n",
    "            group_ids=[\"investment_id\"],\n",
    "            target=\"target\",\n",
    "            time_idx=\"time_id\",\n",
    "            min_encoder_length=max_encoder_length // 2,\n",
    "            max_encoder_length=max_encoder_length,\n",
    "            min_prediction_length=1,\n",
    "            max_prediction_length=max_prediction_length,\n",
    "            static_categoricals=[\"investment_id\"],\n",
    "            static_reals=[],\n",
    "            time_varying_known_categoricals=[],\n",
    "            time_varying_known_reals=df.columns.tolist()[2:-1],\n",
    "            time_varying_unknown_categoricals=[],\n",
    "            time_varying_unknown_reals=['target'],\n",
    "            target_normalizer=GroupNormalizer( # normalize the targe for each investment_id along corresponding time_idx\n",
    "                groups=[\"investment_id\"], \n",
    "                transformation=None # NOTE: do not use softplus or relu for encoder normalization with DeepAR\n",
    "                # transformation=\"softplus\" # NOTE: do not use softplus or relu for encoder normalization with DeepAR\n",
    "            ),\n",
    "            # Add additional features\n",
    "            add_relative_time_idx=True,\n",
    "            add_target_scales=True,\n",
    "            add_encoder_length=True,\n",
    "            allow_missing_timesteps=True,\n",
    "        )\n",
    "        val_dataset = TimeSeriesDataSet.from_dataset(train_dataset, df_val, predict=True, stop_randomization=True)\n",
    "        test_dataset = TimeSeriesDataSet.from_dataset(train_dataset, df_test, predict=True, stop_randomization=True)\n",
    "        # Save dataset to accelerate\n",
    "        train_dataset.save('C:/Users/miaoy/Desktop/11785/HwData/project/pf_train_246_samples_arpca.pf')\n",
    "        val_dataset.save('C:/Users/miaoy/Desktop/11785/HwData/project/pf_val_246_samples_arpca.pf')\n",
    "        test_dataset.save('C:/Users/miaoy/Desktop/11785/HwData/project/pf_test_246_samples_arpca.pf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
