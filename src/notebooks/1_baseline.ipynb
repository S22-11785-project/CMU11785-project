{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 11785\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_dir(dir_to_add):\n",
    "    if not os.path.exists(dir_to_add):\n",
    "        os.makedirs(dir_to_add)\n",
    "\n",
    "pl.seed_everything(11785)\n",
    "\n",
    "\n",
    "NUM_WORKERS = 8                                                  # Use 4 for AWS\n",
    "DIR_PROJECT = '/media/user/12TB1/HanLi/GitHub/CMU11785-project/' # Change this\n",
    "DIR_TRAINED = os.path.join(DIR_PROJECT, 'local_trained')         # Don't change\n",
    "DIR_DATA = os.path.join(DIR_PROJECT, 'src/data')                 # Don't change\n",
    "DIR_LOCAL_DATA = os.path.join(DIR_PROJECT, 'local_data')         # Don't change\n",
    "DIR_BYID = os.path.join(DIR_LOCAL_DATA, 'content/databyid')      # Might need to change\n",
    "\n",
    "create_dir(DIR_TRAINED)\n",
    "create_dir(DIR_LOCAL_DATA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create and save new dataset with 246 train/val samples and 200 test samples...\n",
      "Dataframes read complete, creating TimeSeriesDataSet...\n"
     ]
    }
   ],
   "source": [
    "# Function to create timeseries dataset\n",
    "# TODO: Miao to add PCA and AR pre-processing steps\n",
    "def create_dataset(\n",
    "    ls_train_val_ids,\n",
    "    ls_test_ids, # Should be a subset of ls_train_val_ids\n",
    "    starting_time_id=400, # Use data after 400 time idx\n",
    "    max_prediction_length=3,\n",
    "    max_encoder_length=14,\n",
    "    save_path=os.path.join(DIR_LOCAL_DATA, 'tft_246'),\n",
    "):\n",
    "    \"\"\" Train : Val : Test = 7 : 2 : 1\n",
    "    \"\"\"\n",
    "    assert set(ls_test_ids).issubset(set(ls_train_val_ids))\n",
    "    print(f\"Reading raw data {len(ls_train_val_ids)} train/val samples and {len(ls_test_ids)} test samples...\")\n",
    "    f_cols = [f\"f_{i}\" for i in range(300)]\n",
    "    ls_dfs = []\n",
    "    for id in ls_train_val_ids:\n",
    "        df_f_id = pd.DataFrame(np.load(os.path.join(DIR_BYID, f'feats/{id}.npy')), columns=f_cols)\n",
    "        df_t_id = pd.DataFrame(np.load(os.path.join(DIR_BYID, f'target/{id}.npy')), columns=['target'])\n",
    "        df_f_id['investment_id'] = id\n",
    "        df_id = pd.concat([df_t_id, df_f_id], axis=1)\n",
    "        ls_dfs.append(df_id)\n",
    "\n",
    "    df = pd.concat(ls_dfs).reset_index().rename(columns={'index': 'time_id'})\n",
    "    df = df.sort_values(by=['time_id']) # sort by time before splitting\n",
    "\n",
    "    df = df.loc[df['time_id'] >= starting_time_id]\n",
    "    df_train, df_test = train_test_split(df, test_size=0.1, shuffle=False)\n",
    "    df_train, df_val = train_test_split(df_train, test_size=2/9, shuffle=False)\n",
    "    df_test = df_test.loc[df_test['investment_id'].isin(ls_test_ids)].reset_index(drop=True)\n",
    "    df_train['investment_id'] = df_train['investment_id'].astype(str)\n",
    "    df_val['investment_id'] = df_val['investment_id'].astype(str)\n",
    "    df_test['investment_id'] = df_test['investment_id'].astype(str)\n",
    "\n",
    "    print('Dataframes read complete, creating TimeSeriesDataSet...')\n",
    "    \n",
    "    # create the dataset from the pandas dataframe\n",
    "    train_dataset = TimeSeriesDataSet(\n",
    "        df_train,\n",
    "        group_ids=[\"investment_id\"],\n",
    "        target=\"target\",\n",
    "        time_idx=\"time_id\",\n",
    "        min_encoder_length=max_encoder_length // 2,\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        min_prediction_length=1,\n",
    "        max_prediction_length=max_prediction_length,\n",
    "        static_categoricals=[\"investment_id\"],\n",
    "        static_reals=[],\n",
    "        time_varying_known_categoricals=[],\n",
    "        time_varying_known_reals=f_cols,\n",
    "        time_varying_unknown_categoricals=[],\n",
    "        time_varying_unknown_reals=['target'],\n",
    "        target_normalizer=GroupNormalizer( # normalize the targe for each investment_id along corresponding time_idx\n",
    "            groups=[\"investment_id\"], \n",
    "            transformation=None # NOTE: do not use softplus or relu for encoder normalization with DeepAR\n",
    "            # transformation=\"softplus\" # NOTE: do not use softplus or relu for encoder normalization with DeepAR\n",
    "        ),\n",
    "        # Add additional features\n",
    "        add_relative_time_idx=True,\n",
    "        add_target_scales=True,\n",
    "        add_encoder_length=True,\n",
    "    )\n",
    "    val_dataset = TimeSeriesDataSet.from_dataset(train_dataset, df_val, predict=True, stop_randomization=True)\n",
    "    test_dataset = TimeSeriesDataSet.from_dataset(train_dataset, df_test, predict=True, stop_randomization=True)\n",
    "\n",
    "    if save_path is not None:\n",
    "        print(f\"Save datasets with {len(ls_train_val_ids)} train/val samples and {len(ls_test_ids)} test samples...\")\n",
    "        # Save dataset so we can use it next time\n",
    "        create_dir(save_path)\n",
    "        train_dataset.save(os.path.join(save_path, f'tft_train_{len(ls_train_val_ids)}_samples.pf'))\n",
    "        val_dataset.save(os.path.join(save_path, f'tft_val_{len(ls_train_val_ids)}_samples.pf'))\n",
    "        test_dataset.save(os.path.join(save_path, f'tft_test_{len(ls_test_ids)}_samples.pf'))\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "# Read subset list\n",
    "ls_train_val_ids = pd.read_pickle(os.path.join(DIR_DATA, 'test_246_ids.pkl'))\n",
    "ls_test_ids = random.sample(ls_train_val_ids, 200)\n",
    "train_dataset, val_dataset, test_dataset = create_dataset(ls_train_val_ids, ls_test_ids)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52c4a99fb36d68752ce25c6541fc636e9171dab977cfe863248a143161a3b436"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('11785_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
