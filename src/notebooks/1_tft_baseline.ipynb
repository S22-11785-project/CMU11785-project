{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try TemporalFusionTransformer\n",
    "* Read: https://towardsdatascience.com/temporal-fusion-transformer-a-primer-on-deep-forecasting-in-python-4eb37f3f3594"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DIR_DATA = '/media/user/12TB1/HanLi/GitHub/CMU11785-project/local_data'\n",
    "NUM_WORKERS = 32 # Use 4 for AWS\n",
    "\n",
    "args = {\n",
    "    # ------------------------------\n",
    "    # Basic config\n",
    "    'random_seed': 11785,\n",
    "    'n_samples': 10,\n",
    "    'batch_size': 64,\n",
    "    'n_workers' : NUM_WORKERS,\n",
    "    'criterion': {\n",
    "        'quantile': QuantileLoss(),\n",
    "        'pearson': None,    # TODO: Miao to implement loss class\n",
    "        'other': None,      # TODO: check out other loss\n",
    "    },\n",
    "    # ------------------------------\n",
    "    # Hyperparameters\n",
    "    'lr_s': 2e-1,\n",
    "    'hidden_size': 256,\n",
    "    'attention_head_size': 1,        # use multihead for large hidden size\n",
    "    'dropout': 0.1,\n",
    "    'hidden_continuous_size': 8,     # set to <= hidden_size\n",
    "    'output_size': 7,                # 7 quantiles for QuantileLoss by default\n",
    "    'reduce_on_plateau_patience': 4, # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    'gradient_clip_val': 0.1,\n",
    "    # ------------------------------\n",
    "    # Logging\n",
    "    'log_interval': 5,               # log every n batches, set to None when try to find best lr\n",
    "    'wandb_entity': '11785_project',\n",
    "    'wandb_project': '11785_pf_test',\n",
    "    'wandb_name': 'test_run_1',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, create validation and training dataset\n",
    "dir_pf_dataset = os.path.join(DIR_DATA, 'pf_dataset')\n",
    "n = args['n_samples']\n",
    "\n",
    "train_dataset = TimeSeriesDataSet.load(os.path.join(dir_pf_dataset, f'pf_train_{n}_samples.pf'))\n",
    "val_dataset = TimeSeriesDataSet.load(os.path.join(dir_pf_dataset, f'pf_val_{n}_samples.pf'))\n",
    "val_dataset = TimeSeriesDataSet.load(os.path.join(dir_pf_dataset, f'pf_test_{n}_samples.pf'))\n",
    "\n",
    "# create dataloaders for model\n",
    "train_dataloader = train_dataset.to_dataloader(train=True, batch_size=args['batch_size'], num_workers=args['n_workers'])\n",
    "val_dataloader = val_dataset.to_dataloader(train=False, batch_size=args['batch_size']*5, num_workers=args['n_workers'])\n",
    "\n",
    "print(\"Load existing dataset completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure network and trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['random_seed'] is not None: pl.seed_everything(args['random_seed'])\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    gradient_clip_val=args['gradient_clip_val'],\n",
    ")\n",
    "\n",
    "tft_model = TemporalFusionTransformer.from_dataset(\n",
    "    train_dataset,\n",
    "    learning_rate=args['lr_s'],\n",
    "    hidden_size=args['hidden_size'],  # most important hyperparameter apart from learning rate\n",
    "    attention_head_size=args['attention_head_size'], # number of attention heads. Set to up to 4 for large datasets\n",
    "    dropout=args['dropout'],  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=args['hidden_continuous_size'],\n",
    "    output_size=args['output_size'],\n",
    "    loss=args['criterion']['quantile'],\n",
    "    log_interval=args['log_interval'],  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    reduce_on_plateau_patience=args['reduce_on_plateau_patience'], # reduce learning rate if no improvement in validation loss after x epochs\n",
    ")\n",
    "\n",
    "print(f\"Number of parameters in network: {tft_model.size()/1e3:.1f}k\")\n",
    "\n",
    "# find optimal learning rate\n",
    "res = trainer.tuner.lr_find(\n",
    "    tft_model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52c4a99fb36d68752ce25c6541fc636e9171dab977cfe863248a143161a3b436"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('11785_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
